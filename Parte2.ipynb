{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n",
      "Cantidad clase negativa:  1784\n",
      "Cantidad clase positiva:  1770\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos y ver sus dimensiones\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "\n",
    "# Como se puede observar, la dimensionalidad de la data de entrenamiento y la de prueba es de 3554 registros en total.\n",
    "\n",
    "#Contar cantidad de cada clase\n",
    "con_neg = 0\n",
    "con_pos = 0\n",
    "\n",
    "for val in train_df[\"Sentiment\"]:\n",
    "    if val > 0:\n",
    "        con_pos+=1\n",
    "    else:\n",
    "        con_neg+=1\n",
    "        \n",
    "print \"Cantidad clase negativa: \",con_neg\n",
    "print \"Cantidad clase positiva: \",con_pos\n",
    "\n",
    "# Existen dos clases, textos positivos y negativos. El dataset contiene:\n",
    "\n",
    "# 1784 textos negativos\n",
    "# 1770 textos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------word_extractor---------------------\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " n't love play game\n",
      "---------------------word_extractor_sin_stemming---------------------\n",
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " n't love play game\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "def word_extractor_sin_stemming(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"---------------------word_extractor---------------------\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor(\"I love to play games\")\n",
    "print word_extractor(\"I love playing games\")\n",
    "print word_extractor(\"I loved playing the games\")\n",
    "print word_extractor(\"I do not love playing games\")\n",
    "print word_extractor(\"I don't love playing games\")\n",
    "\n",
    "print \"---------------------word_extractor_sin_stemming---------------------\"\n",
    "print word_extractor_sin_stemming(\"I love to eat cake\")\n",
    "print word_extractor_sin_stemming(\"I love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I loved eating the cake\")\n",
    "print word_extractor_sin_stemming(\"I do not love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor(\"I love to play games\")\n",
    "print word_extractor(\"I love playing games\")\n",
    "print word_extractor(\"I loved playing the games\")\n",
    "print word_extractor(\"I do not love playing games\")\n",
    "print word_extractor(\"I don't love playing games\")\n",
    "\n",
    "\n",
    "# Se puede observar que al aplicar el algoritmo word_extractor() captura el tronco lexico base de cada palabra\n",
    "# en las distintas oraciones. En los 4 primeros ejemplos se obtiene el mismo tronco lexico para las oraciones,\n",
    "# puesto que se trata solamente de palabras que se le agrega el \"ing\" o el \"ed\" al final.\n",
    "# Tambien se observa que existe diferencia entre poner \"do not\" y \"don't\" obteniendose distinto tronco,\n",
    "# porque en el primer caso se consideran palabras separadas como \"do\" y \"not\" por separado.\n",
    "# Se observa el mismo resultado para las oraciones propias.\n",
    "# Si no se aplica stemming, no todas las palabras quedan en su tronco lexico base, solo se extraen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " love play game\n",
      " love playing game\n",
      " loved playing game\n",
      " love playing game\n",
      " n't love playing game\n"
     ]
    }
   ],
   "source": [
    "# Funcion igual a la anterior, pero con lematizing en vez de stemming\n",
    "\n",
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "            for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "\n",
    "#propias\n",
    "print word_extractor2(\"I love to play games\")\n",
    "print word_extractor2(\"I love playing games\")\n",
    "print word_extractor2(\"I loved playing the games\")\n",
    "print word_extractor2(\"I do not love playing games\")\n",
    "print word_extractor2(\"I don't love playing games\")\n",
    "\n",
    "# Como se puede observar, solo se extraen las palabras presentes en la oracion, exactamente igual que en el caso\n",
    "# anterior donde sacabamos la aplicacion del proceso de stemming.\n",
    "# No existen muchas mas diferencias. Simplemente no se llega al tronco lexico que se busca al aplicar stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mas frecuentes en el train: \n",
      "566 film\n",
      "481 movie\n",
      "246 one\n",
      "245 like\n",
      "224 ha\n",
      "183 make\n",
      "176 story\n",
      "163 character\n",
      "145 comedy\n",
      "143 time\n",
      "Palabras mas frecuentes en el test: \n",
      "558 film\n",
      "540 movie\n",
      "250 one\n",
      "238 ha\n",
      "230 like\n",
      "197 story\n",
      "175 character\n",
      "165 time\n",
      "161 make\n",
      "134 comedy\n"
     ]
    }
   ],
   "source": [
    "# Representacion vectorial del texto de entrenamiento y el de pruebas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "#for tag, count in lista_train:\n",
    "#    print count, tag\n",
    "\n",
    "N = 10\n",
    "\n",
    "pals_train = []\n",
    "pals_test = []\n",
    "\n",
    "print \"Palabras mas frecuentes en el train: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (566 film,481 movie,246 one,245 like,224 ha,183 make,176 story,163 character,145 comedy,143 time)\n",
    "    \n",
    "print \"Palabras mas frecuentes en el test: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_test[i]\n",
    "    pals_test.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (558 film,540 movie,250 one,238 ha,230 like,197 story,175 character,165 time,161 make,134 comedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion que evalua el desempeño de un clasificador generico en el conjunto de entrenamiento y de pruebas\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "    \n",
    "# Las metricas de classification_report son:\n",
    "# yt: Corresponde a las y de prueba, es decir, las clasificaciones reales.\n",
    "# model.predict(xt): Corresponde a la prediccion de los inputs \"xt\" de prueba, es decir, el y estimado.\n",
    "# target_names: Corresponde a una lista de strings para mostrar nombres para las etiquetas. En este caso \"+\" y \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f2a1b41609ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_NAIVE_BAYES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "    \n",
    "    \n",
    "# Las StopWords son el nombre que se le da a todas aquellas palabras que no tienen ningún atributo\n",
    "# de búsqueda, es decir, son palabras de significado vacío como los artículos, los pronombres o las preposiciones.\n",
    "# La importancia de borrar estas palabras es para hacer mas eficiente el analisis de clasificacion, puesto que \n",
    "# asi no se pierde tiempo procesando y guardando estas palabras en el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,random\n",
    "\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "start_t = time.time()\n",
    "Cs = [0.01,0.1,10,100,1000]\n",
    "for C in Cs:\n",
    "print \"Usando C= %f\"%C\n",
    "model = LogisticRegression(penalty='l2',C=C)\n",
    "model = model.fit(x, y)\n",
    "score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construir grafico comparativo. Probablemente de errores de entrenamiento y test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

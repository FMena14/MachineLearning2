{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de opiniones sobre Películas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se trabajará con un dataset del sitio <b>Rotten Tomatoes</b>, el cual consta de opiniones de la gente sobre alguna película, en texto en ingles. Se intentará predecir si la opinion de la persona es una opinion $positiva$ o $negativa$ (1 y -1) en base a un análisis de las palabras utilizadas en la opinion. Se trabaja ademas con operaciones de stopwords, stemming y lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n",
      "   Sentiment                                               Text\n",
      "0         -1  everything's serious , poetic , earnest and --...\n",
      "1         -1  narratively , trouble every day is a plodding ...\n",
      "2          1  a truly wonderful tale combined with stunning ...\n",
      "3          1  jason patric and ray liotta make for one splen...\n",
      "4         -1  haneke keeps us at arm's length . guided more ...\n",
      "      Sentiment                                               Text\n",
      "3549          1  a fascinating documentary about the long and e...\n",
      "3550          1  the filmmakers' eye for detail and the high st...\n",
      "3551          1  throwing caution to the wind with an invitatio...\n",
      "3552         -1  �a big , baggy , sprawling carnival of a movie...\n",
      "3553          1  an incendiary , deeply thought-provoking look ...\n",
      "TRAINING-Cantidad clase negativa:  1784\n",
      "TRAINING-Cantidad clase positiva:  1770\n",
      "TEST-Cantidad clase negativa:  1803\n",
      "TEST-Cantidad clase positiva:  1751\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos y ver sus dimensiones\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print train_df.head()\n",
    "print test_df.tail()\n",
    "\n",
    "#Contar cantidad de cada clase   \n",
    "print \"TRAINING-Cantidad clase negativa: \",train_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TRAINING-Cantidad clase positiva: \",train_df[\"Sentiment\"].tolist().count(1)\n",
    "print \"TEST-Cantidad clase negativa: \",test_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TEST-Cantidad clase positiva: \",test_df[\"Sentiment\"].tolist().count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera parte se carga los datos y se almacenan en un archivo local .csv. Luego se muestra un extracto del dataset donde se pueden ver opiniones ($text$) y su polaridad ($sentiment$).\n",
    "\n",
    "La cantidad de datos en el training set de la clase negativa y positiva son 1784 y 1770 respectivamente. La cantidad de datos en el test set de la clase negativa y positiva son 1803 y 1751 respectivamente. Se puede observar que la cantidad de datos por clase es homogéneo, aportando a que el training set tenga un aprendizaje equitativo en cuanto a las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------word_extractor---------------------\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n",
      " walk moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog call pluto\n",
      " eat cake\n",
      " eat lot jelli\n",
      " eat lot cake\n",
      "---------------------word_extractor_sin_stemming---------------------\n",
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " walking moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog called pluto\n",
      " eating cake\n",
      " eat lots jellies\n",
      " eat lots cakes\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stemming(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"---------------------word_extractor---------------------\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "# propias\n",
    "print word_extractor(\"They are walking in the moon\")\n",
    "print word_extractor(\"I see a big foot\")\n",
    "print word_extractor(\"I saw my dog eat my cat\")\n",
    "print word_extractor(\"I have a dog called Pluto\")\n",
    "print word_extractor(\"I am eating cake\")\n",
    "print word_extractor(\"I eat lots of jellies\")\n",
    "print word_extractor(\"I eat a lots of cakes\")\n",
    "\n",
    "print \"---------------------word_extractor_sin_stemming---------------------\"\n",
    "print word_extractor_sin_stemming(\"I love to eat cake\")\n",
    "print word_extractor_sin_stemming(\"I love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I loved eating the cake\")\n",
    "print word_extractor_sin_stemming(\"I do not love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I don't love eating cake\")\n",
    "# propias\n",
    "print word_extractor_sin_stemming(\"They are walking in the moon\")\n",
    "print word_extractor_sin_stemming(\"I see a big foot\")\n",
    "print word_extractor_sin_stemming(\"I saw my dog eat my cat\")\n",
    "print word_extractor_sin_stemming(\"I have a dog called Pluto\")\n",
    "print word_extractor_sin_stemming(\"I am eating cake\")\n",
    "print word_extractor_sin_stemming(\"I eat lots of jellies\")\n",
    "print word_extractor_sin_stemming(\"I eat a lots of cakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que al aplicar el algoritmo $word\\_extractor()$ captura el tronco léxico base de cada palabra en las distintas oraciones. En los 4 primeros ejemplos se obtiene el mismo tronco léxico para las oraciones, puesto que se trata solamente de palabras que se le agrega el \"ing\" o el \"ed\" al final. Tambien se observa que existe diferencia entre poner \"do not\" y \"don't\" obteniéndose distintas palabras como resultado, en el primer caso se consideran palabras separadas por lo que son eliminadas por la función <b>stopwords</b>, la cual elimina palabras que no aportan al significado, es decir, palabras sin información o de significado vacío como los artículos, los pronombres o las preposiciones. La importancia de borrar estas palabras es para hacer más eficiente el análisis de clasificación, puesto que así no se pierde tiempo procesando y guardando estas palabras en el algoritmo.\n",
    "\n",
    "Si no se aplica stemming, las palabras no son reducidas a su tronco léxico por lo que quedan con su \"extención\" (-ing, -s, -ies, -es, -ed, entre otros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " walking moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog called pluto\n",
      " eating cake\n",
      " eat lot jelly\n",
      " eat lot cake\n"
     ]
    }
   ],
   "source": [
    "# Funcion igual a la anterior, pero con lematizing en vez de stemming\n",
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "            for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stop(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    words = \"\"\n",
    "    for word in wordtokens:\n",
    "        words+=\" \"+word\n",
    "    return words\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "#propias\n",
    "print word_extractor2(\"They are walking in the moon\")\n",
    "print word_extractor2(\"I see a big foot\")\n",
    "print word_extractor2(\"I saw my dog eat my cat\")\n",
    "print word_extractor2(\"I have a dog called Pluto\")\n",
    "print word_extractor2(\"I am eating cake\")\n",
    "print word_extractor2(\"I eat lots of jellies\")\n",
    "print word_extractor2(\"I eat a lot of cakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de la función $word\\_extractor2()$ utiliza <i>lemmatization</i> el cual reduce las palabras asociándolas a alguna palabra del diccionario, eliminando los finales de las palabras que cambian el significado de estas, reduciendolas al una forma canónica.\n",
    "\n",
    "Se puede ver la diferencia entre <i>lemmatization</i> y <i>stemming</i> en los ejemplos, ya que la primera deja intacto los finales -ed y -ing. Ambos eliminan los finales -es y -s, como es el caso de \"jellies\" a \"jelly\" y \"cakes\" a \"cake\", sin embargo la forma mediante <i>stemming</i> es mas reducida, ya que elimina también los finales -ed y -ing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras en el diccionario: 9663.000000\n",
      "==========  =========  ============  ===  ======  =========  ============\n",
      "  Training  Palabra      Frecuencia  #      Test  Palabra      Frecuencia\n",
      "==========  =========  ============  ===  ======  =========  ============\n",
      "         1  film                566  #         1  film                558\n",
      "         2  movie               481  #         2  movie               540\n",
      "         3  one                 246  #         3  one                 250\n",
      "         4  like                245  #         4  ha                  238\n",
      "         5  ha                  224  #         5  like                230\n",
      "         6  make                183  #         6  story               197\n",
      "         7  story               176  #         7  character           175\n",
      "         8  character           163  #         8  time                165\n",
      "         9  comedy              145  #         9  make                161\n",
      "        10  time                143  #        10  comedy              134\n",
      "==========  =========  ============  ===  ======  =========  ============\n"
     ]
    }
   ],
   "source": [
    "# Representacion vectorial del texto de entrenamiento y el de pruebas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "vocab = vectorizer.get_feature_names() #se crea en base al texts train\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "N = 10\n",
    "pals_train = []\n",
    "count_train =[]\n",
    "pals_test = []\n",
    "count_test = []\n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    count_train.append(count)\n",
    "    tag_test, count_t = lista_test[i]\n",
    "    pals_test.append(tag_test)\n",
    "    count_test.append(count_t)    \n",
    "print \"Cantidad de palabras en el diccionario: %f\"%(len(vocab))\n",
    "\n",
    "a = [range(1,11),pals_train,count_train, [\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\"], range(1,11), pals_test,count_test]\n",
    "table =  zip(*a)\n",
    "from tabulate import tabulate\n",
    "print tabulate(table, headers=[\"Training\",\"Palabra\",\"Frecuencia\",\"#\", \"Test\",\"Palabra\",\"Frecuencia\"],  tablefmt=\"rst\")\n",
    "a = [pals_test,count_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera una representación vectorial de los datos de entrenamiento y de prueba, ajustado a los datos de entrenamiento, donde cada eje representa a una palabra, indicando un 1 si esa palabra está presente en la opinion de la persona, es decir, es un vector de las palabras que contiene la opinion de la persona.\n",
    "\n",
    "La cantidad de palabras en el diccionario es 9663. Dentro de estas, las más frecuentes para el training set y para el test set son presentadas en la tabla anterior, donde las 10 palabras obtenidas en cada set son las mismas pero con distinta frecuencia. Se puede ver como las palabras más repetidas son las más relacionadas con la temática (películas) ya que son \"film\", \"movie\", \"story\",\"character\",\"comedy\". Hay algunas palabras bastante repetidas que no entregan mucho significado, tal como \"ha\" o como \"one\". 1 de cada 7 opiniones presenta la palabra \"film\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def representacion(forma):\n",
    "    if forma == \"normal\":\n",
    "        texts_train = [word_extractor_sin_stop(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor_sin_stop(text) for text in test_df.Text]\n",
    "    elif forma == \"stem\":\n",
    "        texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "    elif forma == \"lem\":\n",
    "        texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test = vectorizer.transform(texts_test)\n",
    "    labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "    labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "    return features_train,labels_train,features_test,labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior es utilizada para generar la representación vectorial mediante los distintos procesos de reducción léxica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Funcion que evalua el desempeño de un clasificador generico en el conjunto de entrenamiento y de pruebas\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "#Funcion que calcula los errores de un modelo\n",
    "def errors(model,x,y,xt,yt): \n",
    "    yhat = model.predict(x)\n",
    "    yhat_test = model.predict(xt)\n",
    "    error = mis_class(yhat,y)\n",
    "    terror = mis_class(yhat_test,yt)\n",
    "    return error, terror\n",
    "def mis_class(yhat,y):\n",
    "    miss = [ 1 if(i != j) else 0  \n",
    "            for i,j in zip(yhat,y)]\n",
    "    return np.mean(miss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función presentada anteriormente ($classification\\_report()$) calcula 4 métricas (precision, recall, f1-score, support). Cada métrica es calculada de forma independiente para cada clase, donde el significado de <b> precision</b> es una tasa/razón entre los <i>true positive</i> y el resto de los positivos (<i>true positive + false positive</i>), en otras palabras representa la habilidad del clasificador en no etiquetar como clase \"interna\" a una clase \"externa\". El significado de <b>recall</b> tasa/razón entre los <i>true positive</i> y el resto de la clase \"interna\" (<i> true positive + false negative</i>), esto representa la habilidad del clasificador en no dejar fuera los ejemplos de la clase propia, es decir, etiquetar correctamente los de la clase \"interna\" . <b>f1-score</b> realiza un promedio harmónico/ponderado entre las métricas de precision y recall. Finalmente <b>support</b> entrega la cantidad de ejemplos asignadas a cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy BernoulliNB: 0.955262\n",
      "Test Accuracy BernoulliNB: 0.748663\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.76      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.044738\n",
      "Error (Misclassification) Test: 0.251266\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.041362\n",
      "Error (Misclassification) Test: 0.261396\n",
      "WITH STEMMING\n",
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057119\n",
      "Error (Misclassification) Test: 0.252110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "errors_bayes = []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se ajustó un modelo <b>Naive Bayes</b>, el cual presenta sus desempeños en los distintos sets, además de información extra acerca del desempeño en cada clase. Agregando al final el error con función de pérdida <i> misclassification</i> para cada set.\n",
    "\n",
    "<i>stemming</i> tiene un desempeño mejor en el test set, <i>lemmatisation</i> por otro lado se comporta mejor en el training set. En base a la información detallada sobre resultados en el test set se puede ver que <i>stemming</i> tiene un mejor comportamiento, esto es explicado ya que como se vio anteriormente este proceso realiza un corte a la palabra mas brusco, reduciéndola más que <i>lemma</i>, dejando información más significativa en cuanto al léxico (mas \"pura\"). El proceso de <i>lemmatisation</i> al dejar terminaciones como -ing, -ed, produce un sesgo sobre los datos, ya que si está presente una palabra en el training set como \"walking\", el modelo la asignará distinta a la palabra \"walk\", perdiéndo la información de que estas palabras significan lo mismo. Para el caso de <i>stemming</i> a estas palabras le asignará la misma probabilidad ya que son consideradas la misma (\"walk\"). \n",
    "\n",
    "Para el caso de filtrar <b>stopwords</b> en el proceso del modelo este presenta un mejor desempeño sobre el test set, esto es contradictorio a lo esperado ya que se espera que al eliminar estas palabras que no entregan información el modelo pueda predecir con mejor exactitud. Esta contradicción se puede explicar por los supuestos de naive bayes, ya que la probabilidad de clasificar una frase, es la probabilidad independiente de cada una de las palabras presentes en la frase, con esto en mente se pudo tener el caso que palabras sin información como \"and\", \"of\", \"the\", \"or\" ayudan a clasificar correctamente la frase, siendo algo totalmente aleatorio dependiendo del training set, ya que esas palabras podrían ir en cualquier contexto, positivas o negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1115585  0.8884415] interesting both as a historical study and as a tragic love story .\n",
      "\n",
      "[ 0.06890124  0.93109876] the year's happiest surprise , a movie that deals with a real subject in an always surprising way .\n",
      "\n",
      "[ 0.06894851  0.93105149] watching war photographer , you come to believe that nachtwey hates the wars he shows and empathizes with the victims he reveals .\n",
      "\n",
      "[ 0.98249908  0.01750092] anemic , pretentious .\n",
      "\n",
      "[ 0.73498718  0.26501282] though frida is easier to swallow than julie taymor's preposterous titus , the eye candy here lacks considerable brio .\n",
      "\n",
      "[ 0.94243663  0.05756337] the film is reasonably entertaining , though it begins to drag two-thirds through , when the melodramatic aspects start to overtake the comedy .\n",
      "\n",
      "[ 0.55127663  0.44872337] to call this one an eventual cult classic would be an understatement , and woe is the horror fan who opts to overlook this goofily endearing and well-lensed gorefest .\n",
      "\n",
      "[ 0.00590384  0.99409616] a stylish but steady , and ultimately very satisfying , piece of character-driven storytelling .\n",
      "\n",
      "[ 0.14970686  0.85029314] jones has tackled a meaty subject and drawn engaging characters while peppering the pages with memorable zingers .\n",
      "\n",
      "[ 0.40828662  0.59171338] as the two leads , lathan and diggs are charming and have chemistry both as friends and lovers .\n",
      "\n",
      "[ 0.02020607  0.97979393] anchored by a terrific performance by abbass , satin rouge shows that the idea of women's self-actualization knows few continental divides .\n",
      "\n",
      "[ 0.14898291  0.85101709] features what is surely the funniest and most accurate depiction of writer's block ever .\n",
      "\n",
      "[ 0.76332684  0.23667316] michele is a such a brainless flibbertigibbet that it's hard to take her spiritual quest at all seriously .\n",
      "\n",
      "[ 0.03021277  0.96978723] jacquot's rendering of puccini's tale of devotion and double-cross is more than just a filmed opera . in his first stab at the form , jacquot takes a slightly anarchic approach that works only sporadically .\n",
      "\n",
      "[ 0.6608908  0.3391092] there's no denying that burns is a filmmaker with a bright future ahead of him .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver lo de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy MULTINOMIAL: 0.955543\n",
      "Test Accuracy MULTINOMIAL: 0.747537\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.75      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.044457\n",
      "Error (Misclassification) Test: 0.252392\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy MULTINOMIAL: 0.959482\n",
      "Test Accuracy MULTINOMIAL: 0.740782\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.040518\n",
      "Error (Misclassification) Test: 0.259145\n",
      "WITH STEMMING\n",
      "Training Accuracy MULTINOMIAL: 0.942319\n",
      "Test Accuracy MULTINOMIAL: 0.749789\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057681\n",
      "Error (Misclassification) Test: 0.250141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "\n",
    "errors_multi= []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se ajustó un modelo <b>Naive Bayes Multinomial</b>, el cual presenta sus desempeños en los distintos sets, además de información extra acerca del desempeño en cada clase. Agregando al final el error con función de pérdida misclassification para cada set.\n",
    "\n",
    "<i>stemming</i> tiene un desempeño mejor en el test set, <i>lemmatisation</i> por otro lado se comporta mejor en el training set, similar al caso de Naive Bayes discutido anteriormente. Se presentan los mismos argumentos del porqué <i>stemming</i> se comporta mejor sobre el test set, debido al reduce de las palabras a una estructura más pura. <i>lemma</i> se comporta mejor sobre el training set debido a que este set está sesgado hacia la información con las palabras reducidas según <i>lemma</i>.\n",
    "\n",
    "Para este caso el filtrar el eliminado de <b>stopwords</b> también produce un efecto contradictorio a lo esperado, como es el caso de Naive Bayes. Por lo que se piensa que es un suceso casual producido por el training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32421864  0.67578136] this franchise has not spawned a single good film . the crap continues .\n",
      "\n",
      "[ 0.90392584  0.09607416] comes across as a fairly weak retooling .\n",
      "\n",
      "[ 0.64892822  0.35107178] flavorful and romantic , you could call this how martha got her groove back -- assuming , that is , she ever had one to begin with .\n",
      "\n",
      "[ 0.95974086  0.04025914] if swimfan does catch on , it may be because teens are looking for something to make them laugh .\n",
      "\n",
      "[ 0.55947259  0.44052741] the film grows on you . and how .\n",
      "\n",
      "[ 0.92402891  0.07597109] that zhang would make such a strainingly cute film -- with a blind orphan at its center , no less -- indicates where his ambitions have wandered .\n",
      "\n",
      "[ 0.57232973  0.42767027] these people wouldn't know subtle characterization if it put on a giant furry monster costume and then gave them a lapdance .\n",
      "\n",
      "[ 0.52960649  0.47039351] summer's far too fleeting to squander on offal like this .\n",
      "\n",
      "[ 0.58510856  0.41489144] it's so underwritten that you can't figure out just where the other characters , including ana's father and grandfather , come down on the issue of ana's future .\n",
      "\n",
      "[ 0.87382389  0.12617611] without a strong script and energetic acting , dogma films can produce the same sleep-inducing effects as watching your neighbor's home videos .\n",
      "\n",
      "[ 0.84561012  0.15438988] to get at the root psychology of this film would require many sessions on the couch of dr . freud .\n",
      "\n",
      "[ 0.39817709  0.60182291] go see it and enjoy .\n",
      "\n",
      "[ 0.04881163  0.95118837] 'frailty \" starts out like a typical bible killer story , but it turns out to be significantly different ( and better ) than most films with this theme .\n",
      "\n",
      "[ 0.47779644  0.52220356] this is a harrowing movie about how parents know where all the buttons are , and how to push them .\n",
      "\n",
      "[ 0.05934057  0.94065943] stands as a document of what it felt like to be a new yorker -- or , really , to be a human being -- in the weeks after 9/11 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver lo de arriba iwal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFECT OF C IN THE MODEL\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.784468\n",
      "Test Accuracy LOGISTIC: 0.678863\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.67      0.73      0.70      1803\n",
      "          -       0.69      0.63      0.66      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.215532\n",
      "Error (Misclassification) Test: 0.321047\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.892234\n",
      "Test Accuracy LOGISTIC: 0.719111\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.72      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.107766\n",
      "Error (Misclassification) Test: 0.280810\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "Usando C= 10.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.718548\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.281373\n",
      "Usando C= 100.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.714044\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.72      1803\n",
      "          -       0.71      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.285875\n",
      "Usando C= 1000.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.712356\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.287563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,1,1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C= %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        error,terror = errors(model,x,y,xt,yt)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "        print \"Error (Misclassification) Training: %f\"%(error)\n",
    "        print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "def do_LOGIT_C(x,y,xt,yt,c):\n",
    "    start_t = time.time()\n",
    "    model = LogisticRegression(penalty='l2',C=c)\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    error,terror = errors(model,x,y,xt,yt)\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    return error,terror\n",
    "\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso anterior se ajusta un modelo de <b>Regresión Logística Regularizado</b>, donde recibe de parámetro un valor <i>C</i> el cual se analiza su efecto en el desempeño del modelo en el test set y en el training set. Este parámetro <i>C</i> representa el rango de la penalización del modelo a equivocarse sobre el conjunto del cual se entrena, es decir, el error sobre el. Para valores pequeños de <i>C</i> el modelo está permite bajos valores de penalización, por lo que el modelo es mas relajado en permitir un mayor error sobre el training set, mostrándo esto en que el desempeño sobre el training set es el peor. Con un <i>C</i> muy bajo se tiene que el modelo pasa a ser muy relajado por lo que presenta un menor desempeño en el test set. \n",
    "Cuando <i>C</i> es mayor a 10, se pueden ver modelos no relajados, ya que con mayor penalización sobre el error en el training set el modelo se ajusta a este set produciendo un error de entrenamiento de 0 (desempeño 100%). Se puede ver esta transición ya que a medida que el parámetro aumenta el desempeño en el training set también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy LOGISTIC: 0.987901\n",
      "Test Accuracy LOGISTIC: 0.736279\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.74      1803\n",
      "          -       0.73      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.012099\n",
      "Error (Misclassification) Test: 0.263647\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "WITH STEMMING\n",
      "Training Accuracy LOGISTIC: 0.981148\n",
      "Test Accuracy LOGISTIC: 0.735153\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.018852\n",
      "Error (Misclassification) Test: 0.264772\n"
     ]
    }
   ],
   "source": [
    "#SEGUN LO DE ARRIBA SE ESCOGE C = 1\n",
    "errors_logit = []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,1)\n",
    "errors_logit.append([error,terror])\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,1)\n",
    "errors_logit.append([error,terror])\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,1)\n",
    "errors_logit.append([error,terror])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda superior se ajustó un modelo de Regresión Lineal Regularizada con parámetro <i>C </i> igual a 1, mostrando el efecto de utilizar <i>stemming </i> o <i>lemmatisation</i> en la representación de los datos. Para esto se puede ver que la representación mediante <i>stemming</i> se comporta mejor sobre el test set, produciendo un buen desempeño en todas las métricas presentadas anteriormente (precision, recall, f1-score). Esto se explica al igual que en los casos anteriores de Naive Bayes, debido a que este proceso es mejor ya que se trabaja con información pura (palabras en su tronco léxico).\n",
    "\n",
    "El filtrar el proceso de <b>stopwords</b> al igual que en los casos anteriores presenta un mejor comportamiendo sobre el test set, debido al mismo razonamiento de que es producido por que en este caso el training set genera una serie de probabilidades en las palabras que elimina el <b>stopwords</b> las cuales benefician al momento de ver el desempeño sobre el test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFECT OF C IN THE MODEL\n",
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.884637\n",
      "Test Accuracy SVM: 0.715170\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.71      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.989589\n",
      "Test Accuracy SVM: 0.723614\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 1.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.710667\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.70      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.702786\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.698565\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.697439\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.70      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "def do_SVM_C(x,y,xt,yt,c):\n",
    "    model = LinearSVC(C=c)\n",
    "    model = model.fit(x, y)\n",
    "    error,terror = errors(model,x,y,xt,yt)\n",
    "    score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    return error,terror\n",
    "\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para lo presentado en la celda anterior se tiene un modelo <b>SVM</b> el cual es ajustado al training set analizando su comportamiendo en base al parámetro de penalización <i>C</i>, donde este valor representa el error presente en el ajuste del modelo sobre el training set, en otras palabras, representa el grado de suavidad del modelo. Para <i>C</i> pequeños el modelo penaliza menos los errores sobre el ajuste en el trainng set, traduciendose en un modelo mas suave (</i>soft-margin</i>) el cual permite errores sobre los datos en los cuales se ajusta. Para <i>C</i> grande la penalización sobre el error en el conjunto con el cual se entrena es mayor, traduciéndose en un modelo mas estricto (<i>hard-margin</i>) sobre el training set.\n",
    "\n",
    "Para este dataset de opiniones sobre películas se puede ver el efecto esperado del parámetro <i>C</i> sobre el modelo <b>SVM</b>, donde con valores pequeños el desempeño sobre el training set es bajo comparado con otro valor de <i>C</i>. Para <i>C</i> mayor a 1 el modelo es de <i>hard-margin</i>, es decir es estricto sobre el training set, traduciéndose en un desempeño de 100% sobre este set y con un menor desempeño sobre el test set, produciéndo un claro <i>overfitting</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy SVM: 0.987901\n",
      "Test Accuracy SVM: 0.738249\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.012099\n",
      "Error (Misclassification) Test: 0.261677\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy SVM: 0.989589\n",
      "Test Accuracy SVM: 0.723614\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.276308\n",
      "WITH STEMMING\n",
      "Training Accuracy SVM: 0.981992\n",
      "Test Accuracy SVM: 0.731213\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.73      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.018008\n",
      "Error (Misclassification) Test: 0.268711\n"
     ]
    }
   ],
   "source": [
    "#SEGUN LO DE ARRIBA SE ESCOGE C = 0.1\n",
    "errors_svm = []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,0.1)\n",
    "errors_svm.append([error,terror])\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,0.1)\n",
    "errors_svm.append([error,terror])\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,0.1)\n",
    "errors_svm.append([error,terror])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En la celda superior se ajusta un modelo de <b>SVM</b> con parámetro <i>C</i> igual a 0.1 para ver el efecto de realizar distintas representaciones verbales sobre el dataset.\n",
    "\n",
    "blablabla <-- goku explicacion de porke stemming es mejor que lemma.\n",
    "\n",
    "Para el caso de filtrar <b>stopwords</b> en el proceso del modelo este presenta un mejor desempeño sobre el test set, caso contradictorio a lo esperado al igual que en los 3 modelos ya descritos anteriormente, esto deja claro que el training set tiene un sesgo a realizar un ajuste mejor sobre el test set cuando son utilizadas estas palabras que no entrengan información. Un ejemplo de este suceso podría presentarse que en el caso en que el conector \"and\" aparezca en una gran cantidad de opiniones <i>positivas</i>, esto ayudará a que el modelo prediga mejor sobre el test set, pero es un caso específico de los dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAI6CAYAAADVFoJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2ade7c590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Construir grafico comparativo. Probablemente de errores de entrenamiento y test\n",
    "def results_model(model,x,y,xt,yt):\n",
    "    model = model.fit(x, y)\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    return acc_tr,acc_test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "nostop = errors_bayes[0] + errors_multi[0] + errors_logit[0] + errors_svm[0]\n",
    "lem = errors_bayes[0] + errors_multi[1] + errors_logit[1] + errors_svm[1]\n",
    "stem = errors_bayes[2] + errors_multi[2] + errors_logit[2] + errors_svm[2]\n",
    "\n",
    "#PARA COMPARAR LOS EFECTOS DE STEM Y LEM\n",
    "colors = ['b','r','b','r','b','r','b','r']\n",
    "f, axarr = plt.subplots(2, 2, figsize=(12,8) )\n",
    "barlist = axarr[0, 0].bar(range(8), nostop, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 0].set_title('Without stop words and lem')\n",
    "axarr[0, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 0].set_ylabel('Error')\n",
    "axarr[0, 0].legend(barlist, [\"Training\",\"Test\"], loc=\"center right\", fancybox= True)\n",
    "\n",
    "axarr[0, 1].bar(range(8), lem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 1].set_title('With lem')\n",
    "axarr[0, 1].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 1].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 1].set_ylabel('Error')\n",
    "axarr[0, 1].legend(barlist, [\"Training\",\"Test\"], loc=\"center right\", fancybox= True)\n",
    "\n",
    "axarr[1, 0].bar(range(8), stem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[1, 0].set_title('With stem')\n",
    "axarr[1, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[1, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[1, 0].set_ylabel('Error')\n",
    "axarr[1, 0].legend(barlist, [\"Training\",\"Test\"],loc=\"center right\", fancybox= True)\n",
    "\n",
    "\n",
    "f.tight_layout() #separar los subplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En este item se presentan los gráficos que resumen el error según misclassification de todos los modelos ajustados sobre el training set del dataset de <b>Rotten Tomatoes</b>. Donde se asigna un error máximo cuando el modelo se equivoca en predecir una etiqueta, para este caso por ejemplo es cuando el modelo predice una etiqueta como <i>positiva</i> siendo que es <i>negativa</i>. Se puede ver que para el caso en que la representación según  <i>lemmatisation</i>  los modelos Logístico y SVM presentan un <i>overfitting</i>  ya que estos se ajustan mucho al training set, en esta representación el modelo que mejor se comporta presentando el menor error en el test set y con menor overfitting es el de <b>Naive Bayes</b>. Para el caso que se utiliza la representación según <i>stemming</i> los modelos Logístico y SVM también presentan este sobre ajuste sobre el training set. El modelo que presenta mejor test set según esta representación es el <b>Multinomial</b>.\n",
    "\n",
    "hablar algo mas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

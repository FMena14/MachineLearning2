{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n",
      "   Sentiment                                               Text\n",
      "0         -1  everything's serious , poetic , earnest and --...\n",
      "1         -1  narratively , trouble every day is a plodding ...\n",
      "2          1  a truly wonderful tale combined with stunning ...\n",
      "3          1  jason patric and ray liotta make for one splen...\n",
      "4         -1  haneke keeps us at arm's length . guided more ...\n",
      "      Sentiment                                               Text\n",
      "3549          1  a fascinating documentary about the long and e...\n",
      "3550          1  the filmmakers' eye for detail and the high st...\n",
      "3551          1  throwing caution to the wind with an invitatio...\n",
      "3552         -1  �a big , baggy , sprawling carnival of a movie...\n",
      "3553          1  an incendiary , deeply thought-provoking look ...\n",
      "TRAINING-Cantidad clase negativa:  1784\n",
      "TRAINING-Cantidad clase positiva:  1770\n",
      "TEST-Cantidad clase negativa:  1803\n",
      "TEST-Cantidad clase positiva:  1751\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos y ver sus dimensiones\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print train_df.head()\n",
    "print test_df.tail()\n",
    "\n",
    "# Como se puede observar, la dimensionalidad de la data de entrenamiento y la de prueba es de 3554 registros en total.\n",
    "\n",
    "\n",
    "con_neg = 0\n",
    "con_pos = 0\n",
    "\n",
    "for val in train_df[\"Sentiment\"]:\n",
    "    if val > 0:\n",
    "        con_pos+=1\n",
    "    else:\n",
    "        con_neg+=1\n",
    "        \n",
    "#Contar cantidad de cada clase   \n",
    "print \"TRAINING-Cantidad clase negativa: \",train_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TRAINING-Cantidad clase positiva: \",train_df[\"Sentiment\"].tolist().count(1)\n",
    "\n",
    "print \"TEST-Cantidad clase negativa: \",test_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TEST-Cantidad clase positiva: \",test_df[\"Sentiment\"].tolist().count(1)\n",
    "\n",
    "# Existen dos clases, textos positivos y negativos. El dataset contiene:\n",
    "\n",
    "# 1784 textos negativos\n",
    "# 1770 textos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------word_extractor---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:3: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/sparsetools/_graph_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._graph_tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/sparsetools/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._traversal import connected_components\n",
      "/usr/lib/python2.7/dist-packages/sklearn/feature_extraction/hashing.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _hashing\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/extmath.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/extmath.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/usr/lib/python2.7/dist-packages/sklearn/utils/random.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._random import sample_without_replacement\n",
      "/usr/lib/python2.7/dist-packages/sklearn/datasets/svmlight_format.py:25: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._svmlight_format import _load_svmlight_file\n",
      "/usr/lib/python2.7/dist-packages/sklearn/svm/base.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm, liblinear\n",
      "/usr/lib/python2.7/dist-packages/sklearn/svm/base.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import libsvm_sparse\n",
      "/usr/lib/python2.7/dist-packages/scipy/interpolate/interpolate.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _ppoly\n",
      "/usr/lib/python2.7/dist-packages/scipy/spatial/__init__.py:90: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .ckdtree import *\n",
      "/usr/lib/python2.7/dist-packages/scipy/spatial/__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .qhull import *\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/least_angle.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ..utils import array2d, arrayfuncs, as_float_array, check_arrays\n",
      "/usr/lib/python2.7/dist-packages/sklearn/metrics/cluster/supervised.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/usr/lib/python2.7/dist-packages/sklearn/metrics/pairwise.py:56: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import cd_fast\n",
      "/usr/lib/python2.7/dist-packages/sklearn/linear_model/__init__.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/panshop/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-64bb75596882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"---------------------word_extractor---------------------\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love to eat cake\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love eating cake\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I loved eating the cake\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-64bb75596882>\u001b[0m in \u001b[0;36mword_extractor\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcommonwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([a-z])\\1+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr'\\1\\1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#substitute multiple letter by two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/panshop/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stemming(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"---------------------word_extractor---------------------\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor(\"I love to play games\")\n",
    "print word_extractor(\"I love playing games\")\n",
    "print word_extractor(\"I loved playing the games\")\n",
    "print word_extractor(\"I do not love playing games\")\n",
    "print word_extractor(\"I don't love playing games\")\n",
    "print word_extractor(\"I am eating cake\")\n",
    "print word_extractor(\"I'm eating cake\")\n",
    "print word_extractor(\"I eat cake\")\n",
    "\n",
    "print \"---------------------word_extractor_sin_stemming---------------------\"\n",
    "print word_extractor_sin_stemming(\"I love to eat cake\")\n",
    "print word_extractor_sin_stemming(\"I love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I loved eating the cake\")\n",
    "print word_extractor_sin_stemming(\"I do not love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor_sin_stemming(\"I love to play games\")\n",
    "print word_extractor_sin_stemming(\"I love playing games\")\n",
    "print word_extractor_sin_stemming(\"I loved playing the games\")\n",
    "print word_extractor_sin_stemming(\"I do not love playing games\")\n",
    "print word_extractor_sin_stemming(\"I don't love playing games\")\n",
    "print word_extractor_sin_stemming(\"I am eating cake\")\n",
    "print word_extractor_sin_stemming(\"I'm eating cake\")\n",
    "print word_extractor_sin_stemming(\"I eat cake\")\n",
    "\n",
    "\n",
    "# Se puede observar que al aplicar el algoritmo word_extractor() captura el tronco lexico base de cada palabra\n",
    "# en las distintas oraciones. En los 4 primeros ejemplos se obtiene el mismo tronco lexico para las oraciones,\n",
    "# puesto que se trata solamente de palabras que se le agrega el \"ing\" o el \"ed\" al final.\n",
    "# Tambien se observa que existe diferencia entre poner \"do not\" y \"don't\" obteniendose distinto tronco,\n",
    "# porque en el primer caso se consideran palabras separadas como \"do\" y \"not\" por separado.\n",
    "# Se observa el mismo resultado para las oraciones propias.\n",
    "# Si no se aplica stemming, no todas las palabras quedan en su tronco lexico base, solo se extraen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " love play game\n",
      " love playing game\n",
      " loved playing game\n",
      " love playing game\n",
      " n't love playing game\n"
     ]
    }
   ],
   "source": [
    "# Funcion igual a la anterior, pero con lematizing en vez de stemming\n",
    "\n",
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "            for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "\n",
    "#propias\n",
    "print word_extractor2(\"I love to play games\")\n",
    "print word_extractor2(\"I love playing games\")\n",
    "print word_extractor2(\"I loved playing the games\")\n",
    "print word_extractor2(\"I do not love playing games\")\n",
    "print word_extractor2(\"I don't love playing games\")\n",
    "print word_extractor2(\"I am eating cake\")\n",
    "print word_extractor2(\"I'm eating cake\")\n",
    "print word_extractor2(\"I eat cake\")\n",
    "print word_extractor2(\"I will travel the earth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mas frecuentes en el train: \n",
      "566 film\n",
      "481 movie\n",
      "246 one\n",
      "245 like\n",
      "224 ha\n",
      "183 make\n",
      "176 story\n",
      "163 character\n",
      "145 comedy\n",
      "143 time\n",
      "Palabras mas frecuentes en el test: \n",
      "558 film\n",
      "540 movie\n",
      "250 one\n",
      "238 ha\n",
      "230 like\n",
      "197 story\n",
      "175 character\n",
      "165 time\n",
      "161 make\n",
      "134 comedy\n"
     ]
    }
   ],
   "source": [
    "# Representacion vectorial del texto de entrenamiento y el de pruebas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def representacion(forma):\n",
    "    if forma == \"normal\":\n",
    "        texts_train = train_df.Text.aslist()\n",
    "        texts_test = test_df.Text.aslist()\n",
    "    else if forma == \"stem\":\n",
    "        texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "    else if forma == \"lem\":\n",
    "        texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test = vectorizer.transform(texts_test)\n",
    "    labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "    labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "    return features_train,labels_train,features_test,labels_test\n",
    "\n",
    "\n",
    "\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "print vectorizer\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "print dist\n",
    "print dist\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "#for tag, count in lista_train:\n",
    "#    print count, tag\n",
    "\n",
    "N = 10\n",
    "\n",
    "pals_train = []\n",
    "pals_test = []\n",
    "\n",
    "print \"Palabras mas frecuentes en el train: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (566 film,481 movie,246 one,245 like,224 ha,183 make,176 story,163 character,145 comedy,143 time)\n",
    "    \n",
    "print \"Palabras mas frecuentes en el test: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_test[i]\n",
    "    pals_test.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (558 film,540 movie,250 one,238 ha,230 like,197 story,175 character,165 time,161 make,134 comedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion que evalua el desempeño de un clasificador generico en el conjunto de entrenamiento y de pruebas\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "    \n",
    "# Las metricas de classification_report son:\n",
    "# yt: Corresponde a las y de prueba, es decir, las clasificaciones reales.\n",
    "# model.predict(xt): Corresponde a la prediccion de los inputs \"xt\" de prueba, es decir, el y estimado.\n",
    "# target_names: Corresponde a una lista de strings para mostrar nombres para las etiquetas. En este caso \"+\" y \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f2a1b41609ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_NAIVE_BAYES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, random\n",
    "\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "#WITH LEMMATISATION\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "#WITHOUT STOP WORDS and LEMM/STEMM\n",
    "#texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "#texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "texts_train = train_df.Text.aslist()\n",
    "texts_test = train_df.Text.aslist()\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "print vectorizer\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "\n",
    "#ERRORES\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "\n",
    "#WITH STEMMING\n",
    "texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "print vectorizer\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "\n",
    "#ERRORES\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "    \n",
    "# Las StopWords son el nombre que se le da a todas aquellas palabras que no tienen ningún atributo\n",
    "# de búsqueda, es decir, son palabras de significado vacío como los artículos, los pronombres o las preposiciones.\n",
    "# La importancia de borrar estas palabras es para hacer mas eficiente el analisis de clasificacion, puesto que \n",
    "# asi no se pierde tiempo procesando y guardando estas palabras en el algoritmo.\n",
    "\n",
    "#Precision tasa/razon entre los true positive y el resto (true positive + false positive), es decir, clases asignadas correctamente\n",
    "#Recall tasa/razon entre los true positive y el resto del real positivo (true positive + false negative)\n",
    "#f1-score el promedio harmonico/ponderado entre precision y recall\n",
    "#support cantidad de ejemplos asignadas a cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,random\n",
    "\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C= %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'do_NAIVE_BAYES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-49792082ea9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdo_NAIVE_BAYES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdo_MULTINOMIAL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdo_LOGIT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'do_NAIVE_BAYES' is not defined"
     ]
    }
   ],
   "source": [
    "#Construir grafico comparativo. Probablemente de errores de entrenamiento y test\n",
    "\n",
    "def results_model(model,x,y,xt,yt):\n",
    "    model = model.fit(x, y)\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    return acc_tr,acc_test\n",
    "\n",
    "\n",
    "do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "a = [0.5,0.2,0.5,0.6,0.1,0.6,0.1,0.1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "barlist= ax.bar(range(8), a, width  = 0.5, align = \"center\")\n",
    "barlist[1].set_color('r')\n",
    "barlist[3].set_color('r')\n",
    "barlist[5].set_color('r')\n",
    "barlist[7].set_color('r')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Scores para cada clasificador')\n",
    "ax.set_xticks([0.5,2.5,4.5,7.5])\n",
    "ax.set_xticklabels(('Naive Bayes', 'Multinomial', 'Logit', 'SVM'))\n",
    "ax.legend( barlist,[\"Training set\",\"Test set\"], loc = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

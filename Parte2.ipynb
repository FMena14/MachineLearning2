{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de opiniones sobre Películas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se trabajará con un dataset del sitio <b>Rotten Tomatoes</b>, el cual consta de opiniones verbales de la gente sobre alguna película, en texto en inglés. Se intentará predecir si la opinión de la persona es una opinión $positiva$ o $negativa$ (1 y -1) en base a un análisis de las palabras utilizadas en la opinión. Se trabaja además con operaciones de stopwords, stemming y lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n",
      "   Sentiment                                               Text\n",
      "0         -1  everything's serious , poetic , earnest and --...\n",
      "1         -1  narratively , trouble every day is a plodding ...\n",
      "2          1  a truly wonderful tale combined with stunning ...\n",
      "3          1  jason patric and ray liotta make for one splen...\n",
      "4         -1  haneke keeps us at arm's length . guided more ...\n",
      "      Sentiment                                               Text\n",
      "3549          1  a fascinating documentary about the long and e...\n",
      "3550          1  the filmmakers' eye for detail and the high st...\n",
      "3551          1  throwing caution to the wind with an invitatio...\n",
      "3552         -1  �a big , baggy , sprawling carnival of a movie...\n",
      "3553          1  an incendiary , deeply thought-provoking look ...\n",
      "TRAINING-Cantidad clase negativa:  1784\n",
      "TRAINING-Cantidad clase positiva:  1770\n",
      "TEST-Cantidad clase negativa:  1803\n",
      "TEST-Cantidad clase positiva:  1751\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos y ver sus dimensiones\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print train_df.head()\n",
    "print test_df.tail()\n",
    "\n",
    "#Contar cantidad de cada clase   \n",
    "print \"TRAINING-Cantidad clase negativa: \",train_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TRAINING-Cantidad clase positiva: \",train_df[\"Sentiment\"].tolist().count(1)\n",
    "print \"TEST-Cantidad clase negativa: \",test_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TEST-Cantidad clase positiva: \",test_df[\"Sentiment\"].tolist().count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera parte se carga los datos y se almacenan en un archivo local .csv. Luego se muestra un extracto del dataset donde se pueden ver opiniones ($text$) y su polaridad ($sentiment$).\n",
    "\n",
    "La cantidad de datos en el training set de la clase negativa y positiva son 1784 y 1770 respectivamente. La cantidad de datos en el test set de la clase negativa y positiva son 1803 y 1751 respectivamente. Se puede observar que la cantidad de datos por clase es homogéneo, aportando a que el training set tenga un aprendizaje equitativo en cuanto a las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------word_extractor---------------------\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n",
      " walk moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog call pluto\n",
      " eat cake\n",
      " eat lot jelli\n",
      " eat lot cake\n",
      "---------------------word_extractor_sin_stemming---------------------\n",
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " walking moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog called pluto\n",
      " eating cake\n",
      " eat lots jellies\n",
      " eat lots cakes\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stemming(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"---------------------word_extractor---------------------\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "# propias\n",
    "print word_extractor(\"They are walking in the moon\")\n",
    "print word_extractor(\"I see a big foot\")\n",
    "print word_extractor(\"I saw my dog eat my cat\")\n",
    "print word_extractor(\"I have a dog called Pluto\")\n",
    "print word_extractor(\"I am eating cake\")\n",
    "print word_extractor(\"I eat lots of jellies\")\n",
    "print word_extractor(\"I eat a lots of cakes\")\n",
    "\n",
    "print \"---------------------word_extractor_sin_stemming---------------------\"\n",
    "print word_extractor_sin_stemming(\"I love to eat cake\")\n",
    "print word_extractor_sin_stemming(\"I love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I loved eating the cake\")\n",
    "print word_extractor_sin_stemming(\"I do not love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I don't love eating cake\")\n",
    "# propias\n",
    "print word_extractor_sin_stemming(\"They are walking in the moon\")\n",
    "print word_extractor_sin_stemming(\"I see a big foot\")\n",
    "print word_extractor_sin_stemming(\"I saw my dog eat my cat\")\n",
    "print word_extractor_sin_stemming(\"I have a dog called Pluto\")\n",
    "print word_extractor_sin_stemming(\"I am eating cake\")\n",
    "print word_extractor_sin_stemming(\"I eat lots of jellies\")\n",
    "print word_extractor_sin_stemming(\"I eat a lots of cakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que al aplicar el algoritmo $word\\_extractor()$ captura el tronco léxico base de cada palabra en las distintas oraciones. En los 4 primeros ejemplos se obtiene el mismo tronco léxico para las oraciones, puesto que se trata solamente de palabras que se le agrega el \"ing\" o el \"ed\" al final. También se observa que existe diferencia entre poner \"do not\" y \"don't\" obteniéndose distintas palabras como resultado, en el primer caso se consideran palabras separadas por lo que son eliminadas por la función <b>stopwords</b>, la cual elimina palabras que no aportan al significado, es decir, palabras sin información o de significado vacío como los artículos, los pronombres o las preposiciones. La importancia de borrar estas palabras es para hacer más eficiente el análisis de clasificación, puesto que así no se pierde tiempo procesando y guardando estas palabras en el algoritmo.\n",
    "\n",
    "Si no se aplica <i>stemming</i>, las palabras no son reducidas a su tronco léxico por lo que quedan con su \"extensión\" (-ing, -s, -ies, -es, -ed, entre otros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " walking moon\n",
      " see big foot\n",
      " saw dog eat cat\n",
      " dog called pluto\n",
      " eating cake\n",
      " eat lot jelly\n",
      " eat lot cake\n"
     ]
    }
   ],
   "source": [
    "# Funcion igual a la anterior, pero con lematizing en vez de stemming\n",
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "            for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stop(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    words = \"\"\n",
    "    for word in wordtokens:\n",
    "        words+=\" \"+word\n",
    "    return words\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "#propias\n",
    "print word_extractor2(\"They are walking in the moon\")\n",
    "print word_extractor2(\"I see a big foot\")\n",
    "print word_extractor2(\"I saw my dog eat my cat\")\n",
    "print word_extractor2(\"I have a dog called Pluto\")\n",
    "print word_extractor2(\"I am eating cake\")\n",
    "print word_extractor2(\"I eat lots of jellies\")\n",
    "print word_extractor2(\"I eat a lot of cakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de la función $word\\_extractor2()$ utiliza <i>lemmatization</i> el cual reduce las palabras, asociándose a alguna palabra del diccionario, eliminando los finales de las palabras que cambian el significado de estas y reduciendolas a una forma canónica.\n",
    "\n",
    "Se puede ver la diferencia entre <i>lemmatization</i> y <i>stemming</i> en los ejemplos, ya que la primera deja intacto los finales -ed y -ing. Ambos eliminan los finales -es y -s, como es el caso de \"jellies\" a \"jelly\" y \"cakes\" a \"cake\", sin embargo la forma mediante <i>stemming</i> es más reducida, ya que elimina también los finales -ed y -ing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras en el diccionario: 9663.000000\n",
      "==========  =========  ============  ===  ======  =========  ============\n",
      "  Training  Palabra      Frecuencia  #      Test  Palabra      Frecuencia\n",
      "==========  =========  ============  ===  ======  =========  ============\n",
      "         1  film                566  #         1  film                558\n",
      "         2  movie               481  #         2  movie               540\n",
      "         3  one                 246  #         3  one                 250\n",
      "         4  like                245  #         4  ha                  238\n",
      "         5  ha                  224  #         5  like                230\n",
      "         6  make                183  #         6  story               197\n",
      "         7  story               176  #         7  character           175\n",
      "         8  character           163  #         8  time                165\n",
      "         9  comedy              145  #         9  make                161\n",
      "        10  time                143  #        10  comedy              134\n",
      "==========  =========  ============  ===  ======  =========  ============\n"
     ]
    }
   ],
   "source": [
    "# Representacion vectorial del texto de entrenamiento y el de pruebas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "vocab = vectorizer.get_feature_names() #se crea en base al texts train\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "N = 10\n",
    "pals_train = []\n",
    "count_train =[]\n",
    "pals_test = []\n",
    "count_test = []\n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    count_train.append(count)\n",
    "    tag_test, count_t = lista_test[i]\n",
    "    pals_test.append(tag_test)\n",
    "    count_test.append(count_t)    \n",
    "print \"Cantidad de palabras en el diccionario: %f\"%(len(vocab))\n",
    "\n",
    "a = [range(1,11),pals_train,count_train, [\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\",\"#\"], range(1,11), pals_test,count_test]\n",
    "table =  zip(*a)\n",
    "from tabulate import tabulate\n",
    "print tabulate(table, headers=[\"Training\",\"Palabra\",\"Frecuencia\",\"#\", \"Test\",\"Palabra\",\"Frecuencia\"],  tablefmt=\"rst\")\n",
    "a = [pals_test,count_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera una representación vectorial de los datos de entrenamiento y de prueba, ajustado a los datos de entrenamiento, donde cada eje representa a una palabra, indicando un 1 si esa palabra está presente en la opinión de la persona, es decir, es un vector de las palabras que contiene la opinión de la persona.\n",
    "\n",
    "Se puede ver que el <b>diccionario</b> con el cual se trabajará (<i>vocab</i>) es dependiente del training set, por lo que dependerá en gran parte de la diversidad de palabras que se utilicen en este set, debiendo presentar una gran cantidad de datos y con una variedad de lenguaje lo más amplia posible, así para que no existan palabras en el test set que no las reconozca el diccionario ya que no están presentes.\n",
    "\n",
    "La cantidad de palabras en el diccionario es 9663. Dentro de estas, las más frecuentes para el training set y para el test set son presentadas en la tabla anterior, donde las 10 palabras obtenidas en cada set son las mismas pero con distinta frecuencia. Se puede ver como las palabras más repetidas son las más relacionadas con la temática (películas) ya que son \"film\", \"movie\", \"story\",\"character\",\"comedy\". Hay algunas palabras bastante repetidas que no entregan mucho significado, tal como \"ha\" o como \"one\". 1 de cada 7 opiniones presenta la palabra \"film\". Se puede ver como una palabra por si sola no entrega el suficiente significado para determinar si una opinión es positiva o negativa, ya que estas 10 palabras más repetidas son un ejemplo de que no lo hacen. Se tiene el ejemplo de la palabra \"like\" la cual en un contexto distinto es utilizado con distintos propósitos, tales como \"I like the movie\" o \"I didn't like the movie\", por lo que por sí sola no entrega la información necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def representacion(forma):\n",
    "    if forma == \"normal\":\n",
    "        texts_train = [word_extractor_sin_stop(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor_sin_stop(text) for text in test_df.Text]\n",
    "    elif forma == \"stem\":\n",
    "        texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "    elif forma == \"lem\":\n",
    "        texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test = vectorizer.transform(texts_test)\n",
    "    labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "    labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "    return features_train,labels_train,features_test,labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior es utilizada para generar la representación vectorial mediante los distintos procesos de reducción léxica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "# Funcion que evalua el desempeño de un clasificador generico en el conjunto de entrenamiento y de pruebas\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    \n",
    "#Funcion que calcula los errores de un modelo\n",
    "def errors(model,x,y,xt,yt): \n",
    "    yhat = model.predict(x)\n",
    "    yhat_test = model.predict(xt)\n",
    "    error = mis_class(yhat,y)\n",
    "    terror = mis_class(yhat_test,yt)\n",
    "    return error, terror\n",
    "def mis_class(yhat,y):\n",
    "    miss = [ 1 if(i != j) else 0  \n",
    "            for i,j in zip(yhat,y)]\n",
    "    return np.mean(miss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función presentada anteriormente ($classification\\_report()$) calcula 4 métricas (precision, recall, f1-score, support). Cada métrica es calculada de forma independiente para cada clase, donde el significado de <b> precision</b> es una tasa/razón entre los <i>true positive</i> y el resto de los positivos (<i>true positive + false positive</i>), en otras palabras representa la habilidad del clasificador en no etiquetar como clase \"interna\" a una clase \"externa\". El significado de <b>recall</b> tasa/razón entre los <i>true positive</i> y el resto de la clase \"interna\" (<i> true positive + false negative</i>), esto representa la habilidad del clasificador en no dejar fuera los ejemplos de la clase propia, es decir, etiquetar correctamente los de la clase \"interna\" . <b>f1-score</b> realiza un promedio harmónico/ponderado entre las métricas de precision y recall. Finalmente <b>support</b> entrega la cantidad de ejemplos asignadas a cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy BernoulliNB: 0.955262\n",
      "Test Accuracy BernoulliNB: 0.748663\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.76      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.044738\n",
      "Error (Misclassification) Test: 0.251266\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.041362\n",
      "Error (Misclassification) Test: 0.261396\n",
      "WITH STEMMING\n",
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057119\n",
      "Error (Misclassification) Test: 0.252110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "errors_bayes = []\n",
    "accuracy_bayes = []\n",
    "\n",
    "tipos = [\"normal\",\"lem\",\"stem\"]\n",
    "descripcion = [\"WITHOUT STOP WORDS and WITH LEMM\", \"WITH LEMMATISATION\",\"WITH STEMMING\"]\n",
    "for (r,d) in zip(tipos,descripcion):\n",
    "    print d\n",
    "    features_train,labels_train,features_test,labels_test = representacion(r)\n",
    "    model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "    \n",
    "    #calculan y guardan los errores de entrenamiento y prueba\n",
    "    error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "    errors_bayes.append([error,terror])\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    \n",
    "    #valores de retornos agregados por datos requeridos en grafico\n",
    "    acc = model.score(features_test,labels_test)\n",
    "    acct = model.score(features_train,labels_train)\n",
    "    prec_rec = precision_recall_fscore_support(labels_test,model.predict(features_test))[:1]\n",
    "    datos = [acc,acct] + list(prec_rec[0])\n",
    "    accuracy_bayes.append(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se ajustó un modelo <b>Naive Bayes</b>, el cual presenta sus desempeños en los distintos sets, además de información extra acerca del desempeño en cada clase. Agregando al final el error con función de pérdida <i> misclassification</i> para cada set.\n",
    "\n",
    "<i>stemming</i> tiene un desempeño mejor en el test set, entregando una precisión del <b>74,78 %</b>, por otro lado <i>lemmatisation</i> por otro lado se comporta mejor en el training set. En base a la información detallada sobre resultados en el test set se puede ver que <i>stemming</i> tiene un mejor comportamiento, esto es explicado ya que como se vio anteriormente este proceso realiza un corte a la palabra más brusco, reduciéndose más que <i>lemma</i> y dejando información más significativa en cuanto al léxico (más \"pura\"). El proceso de <i>lemmatisation</i> al dejar terminaciones como -ing, -ed, produce un sesgo sobre los datos, ya que si está presente una palabra en el training set como \"walking\", el modelo la asignará distinta a la palabra \"walk\", perdiendo la información de que estas palabras significan lo mismo. Para el caso de <i>stemming</i> a estas palabras le asignará la misma probabilidad ya que son consideradas la misma (\"walk\").\n",
    "\n",
    "Para el caso de filtrar <b>stopwords</b> en el proceso del modelo este presenta un mejor desempeño sobre el test set, esto es contradictorio a lo esperado ya que se espera que al eliminar estas palabras que no entregan información el modelo pueda predecir con mejor exactitud. Esta contradicción se puede explicar por los supuestos de naive bayes, ya que la probabilidad de clasificar una frase, es la probabilidad independiente de cada una de las palabras presentes en la frase, con esto en mente se pudo tener el caso que palabras sin información como \"and\", \"of\", \"the\", \"or\" ayudan a clasificar correctamente la frase, siendo algo totalmente aleatorio dependiendo del training set, ya que esas palabras podrían ir en cualquier contexto, positivas o negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19873416  0.80126584] that rare movie that works on any number of levels -- as a film of magic and whimsy for children , a heartfelt romance for teenagers and a compelling argument about death , both pro and con , for adults .\n",
      "\n",
      "[ 0.87584605  0.12415395] even when crush departs from the 4w formula . . . it feels like a glossy rehash .\n",
      "\n",
      "[ 0.98358017  0.01641983] femme fatale offers nothing more than a bait-and-switch that is beyond playing fair with the audience . are we dealing with dreams , visions or being told what actually happened as if it were the third ending of clue ?\n",
      "\n",
      "[ 0.68851829  0.31148171] stale and clich�d to a fault .\n",
      "\n",
      "[ 0.31989278  0.68010722] for a shoot-'em-up , ballistic is oddly lifeless .\n",
      "\n",
      "[ 0.00528068  0.99471932] imagine o . henry's <b>the gift of the magi</b> relocated to the scuzzy underbelly of nyc's drug scene . merry friggin' christmas !\n",
      "\n",
      "[ 0.01360913  0.98639087] with a cast that includes some of the top actors working in independent film , lovely & amazing involves us because it is so incisive , so bleakly amusing about how we go about our lives .\n",
      "\n",
      "[ 0.46938404  0.53061596] the lousy lead performances . . . keep the movie from ever reaching the comic heights it obviously desired .\n",
      "\n",
      "[ 0.87936956  0.12063044] with a story as bizarre and mysterious as this , you don't want to be worrying about whether the ineffectual broomfield is going to have the courage to knock on that door .\n",
      "\n",
      "[ 0.00929117  0.99070883] despite its shortcomings , girls can't swim represents an engaging and intimate first feature by a talented director to watch , and it's a worthy entry in the french coming-of-age genre .\n",
      "\n",
      "[ 0.64291239  0.35708761] raimi and his team couldn't have done any better in bringing the story of spider-man to the big screen .\n",
      "\n",
      "[ 0.95865525  0.04134475] feel bad for king , who's honestly trying , and schwartzman , who's shot himself in the foot .\n",
      "\n",
      "[ 0.4203618  0.5796382] a trashy , exploitative , thoroughly unpleasant experience .\n",
      "\n",
      "[ 0.49841638  0.50158362] some body will take you places you haven't been , and also places you have .\n",
      "\n",
      "[ 0.02941328  0.97058672] the film doesn't show enough of the creative process or even of what was created for the non-fan to figure out what makes wilco a big deal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que existen ciertas opiniones que solo relatan hechos, sin tener una inclinación positiva o negativa a las cuales generalmente se les asigna una probabilidad equitativa entre las etiquetas de las clases. En otros casos se muestra una clara inclinación hacia una opinión positiva o negativa y el clasificador asigna la probabilidad correspondiente. En general el clasificador Naive Bayes asigna probabilidades correctas a cada opinión, basándose en el producto independiente de la probabilidad de cada palabra que aparece en la opinión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy MULTINOMIAL: 0.955543\n",
      "Test Accuracy MULTINOMIAL: 0.747537\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.75      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.044457\n",
      "Error (Misclassification) Test: 0.252392\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy MULTINOMIAL: 0.959482\n",
      "Test Accuracy MULTINOMIAL: 0.740782\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.040518\n",
      "Error (Misclassification) Test: 0.259145\n",
      "WITH STEMMING\n",
      "Training Accuracy MULTINOMIAL: 0.942319\n",
      "Test Accuracy MULTINOMIAL: 0.749789\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057681\n",
      "Error (Misclassification) Test: 0.250141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "errors_multi= []\n",
    "accuracy_multi = []\n",
    "\n",
    "tipos = [\"normal\",\"lem\",\"stem\"]\n",
    "descripcion = [\"WITHOUT STOP WORDS and WITH LEMM\", \"WITH LEMMATISATION\",\"WITH STEMMING\"]\n",
    "for (r,d) in zip(tipos,descripcion):\n",
    "    print d\n",
    "    features_train,labels_train,features_test,labels_test = representacion(r) #representacion\n",
    "    model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test) #se ejecuta el modelo\n",
    "    \n",
    "    #calculan y guardan los errores de entrenamiento y prueba\n",
    "    error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "    errors_multi.append([error,terror])\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    \n",
    "    #valores de retornos agregados por datos requeridos en grafico\n",
    "    acc = model.score(features_test,labels_test)\n",
    "    acct = model.score(features_train,labels_train)\n",
    "    prec_rec = precision_recall_fscore_support(labels_test,model.predict(features_test))[:1]\n",
    "    datos = [acc,acct] + list(prec_rec[0])\n",
    "    accuracy_multi.append(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se ajustó un modelo <b>Naive Bayes Multinomial</b>, el cual presenta sus desempeños en los distintos sets, además de información extra acerca del desempeño en cada clase. Agregando al final el error con función de pérdida misclassification para cada set.\n",
    "\n",
    "<i>stemming</i> tiene un desempeño mejor en el test set entregando una precisión del <b>74,98 %</b>, <i>lemmatisation</i> por otro lado se comporta mejor en el training set, similar al caso de Naive Bayes discutido anteriormente. Se presentan los mismos argumentos del porqué <i>stemming</i> se comporta mejor sobre el test set, debido al \"reduce\" de las palabras a una estructura más pura. <i>lemma</i> se comporta mejor sobre el training set debido a que este set está sesgado hacia la información con las palabras reducidas según <i>lemma</i>.\n",
    "\n",
    "Para este caso el filtrar el eliminado de <b>stopwords</b> también produce un efecto contradictorio a lo esperado, como es el caso de Naive Bayes. Por lo que se piensa que es un suceso casual producido por el training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02331483  0.97668517] without resorting to hyperbole , i can state that kissing jessica stein may be the best same-sex romance i have seen .\n",
      "\n",
      "[ 0.69856632  0.30143368] though the film is well-intentioned , one could rent the original and get the same love story and parable .\n",
      "\n",
      "[ 0.56274992  0.43725008] follows the original film virtually scene for scene and yet manages to bleed it almost completely dry of humor , verve and fun .\n",
      "\n",
      "[ 0.82423911  0.17576089] despite its dry wit and compassion , the film suffers from a philosophical emptiness and maddeningly sedate pacing .\n",
      "\n",
      "[ 0.76846753  0.23153247] michele is a such a brainless flibbertigibbet that it's hard to take her spiritual quest at all seriously .\n",
      "\n",
      "[ 0.7578147  0.2421853] soul is what's lacking in every character in this movie and , subsequently , the movie itself .\n",
      "\n",
      "[ 0.76146536  0.23853464] trapped won't score points for political correctness , but it may cause parents a few sleepless hours -- a sign of its effectiveness .\n",
      "\n",
      "[ 0.83567133  0.16432867] a serious movie with serious ideas . but seriously , folks , it doesn't work .\n",
      "\n",
      "[ 0.78000444  0.21999556] the sight of the spaceship on the launching pad is duly impressive in imax dimensions , as are shots of the astronauts floating in their cabins .\n",
      "\n",
      "[ 0.22154787  0.77845213] thoroughly engrossing and ultimately tragic .\n",
      "\n",
      "[ 0.02611521  0.97388479] binoche and magimel are perfect in these roles .\n",
      "\n",
      "[  3.09691360e-04   9.99690309e-01] a psychological thriller with a genuinely spooky premise and an above-average cast , actor bill paxton's directing debut is a creepy slice of gothic rural americana .\n",
      "\n",
      "[ 0.60036479  0.39963521] in between the icy stunts , the actors spout hilarious dialogue about following your dream and 'just letting the mountain tell you what to do . '\n",
      "\n",
      "[  3.93330094e-04   9.99606670e-01] a densely constructed , highly referential film , and an audacious return to form that can comfortably sit among jean-luc godard's finest work .\n",
      "\n",
      "[ 0.46450671  0.53549329] director chris eyre is going through the paces again with his usual high melodramatic style of filmmaking .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ver lo de arriba iwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFECT OF C IN THE MODEL\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.784468\n",
      "Test Accuracy LOGISTIC: 0.678863\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.67      0.73      0.70      1803\n",
      "          -       0.69      0.63      0.66      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.215532\n",
      "Error (Misclassification) Test: 0.321047\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.892234\n",
      "Test Accuracy LOGISTIC: 0.719111\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.72      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.107766\n",
      "Error (Misclassification) Test: 0.280810\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "Usando C= 10.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.718548\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.281373\n",
      "Usando C= 100.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.714044\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.72      1803\n",
      "          -       0.71      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.285875\n",
      "Usando C= 1000.000000\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.712356\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.287563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,1,1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C= %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "        #Se calculan y muestran los errores\n",
    "        error,terror = errors(model,x,y,xt,yt)\n",
    "        print \"Error (Misclassification) Training: %f\"%(error)\n",
    "        print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso anterior se ajusta un modelo de <b>Regresión Logística Regularizado</b>, donde recibe de parámetro un valor <i>C</i> el cual se analiza su efecto en el desempeño del modelo en el test set y en el training set. Este parámetro <i>C</i> representa el rango de la penalización del modelo a equivocarse sobre el conjunto del cual se entrena, es decir, el error sobre el. Para valores pequeños de <i>C</i> el modelo está permite bajos valores de penalización, por lo que el modelo es más relajado en permitir un mayor error sobre el training set, mostrando esto en que el desempeño sobre el training set es el peor. Con un <i>C</i> muy bajo se tiene que el modelo pasa a ser muy relajado por lo que presenta un menor desempeño en el test set.\n",
    "Cuando <i>C</i> es mayor a 10, se pueden ver modelos no relajados, es decir con una mayor penalización sobre el error en el training set, por lo que el modelo se ajusta a este set produciendo un error de entrenamiento de 0 (desempeño 100%). Se puede visualizar esta transición en lo mostrado anteriormente, ya que a medida que aumenta el parámetro, el desempeño en el training set también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy LOGISTIC: 0.987901\n",
      "Test Accuracy LOGISTIC: 0.736279\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.74      1803\n",
      "          -       0.73      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.012099\n",
      "Error (Misclassification) Test: 0.263647\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.278559\n",
      "WITH STEMMING\n",
      "Training Accuracy LOGISTIC: 0.981148\n",
      "Test Accuracy LOGISTIC: 0.735153\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.018852\n",
      "Error (Misclassification) Test: 0.264772\n"
     ]
    }
   ],
   "source": [
    "#SEGUN LO DE ARRIBA SE ESCOGE C = 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT_C(x,y,xt,yt,c=1):\n",
    "    start_t = time.time()\n",
    "    model = LogisticRegression(penalty='l2',C=c)\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    return model\n",
    "\n",
    "errors_logit = []\n",
    "accuracy_logit = []\n",
    "\n",
    "tipos = [\"normal\",\"lem\",\"stem\"]\n",
    "descripcion = [\"WITHOUT STOP WORDS and WITH LEMM\", \"WITH LEMMATISATION\",\"WITH STEMMING\"]\n",
    "for r, d in zip(tipos,descripcion): #for para mostrar el efecto de distintas representaciones\n",
    "    print d\n",
    "    features_train,labels_train,features_test,labels_test =  representacion(r)\n",
    "    model = do_LOGIT_C(features_train,labels_train,features_test,labels_test) #se ajusta un modelo LOGISTICO\n",
    "    \n",
    "    #calculan y guardan los errores de entrenamiento y prueba\n",
    "    error,terror = errors(model,features_train,labels_train,features_test,labels_test) \n",
    "    errors_logit.append([error,terror])\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    \n",
    "    #valores de retornos agregados por datos requeridos en grafico\n",
    "    acc = model.score(features_test,labels_test)\n",
    "    acct = model.score(features_train,labels_train)\n",
    "    prec_rec = precision_recall_fscore_support(labels_test,model.predict(features_test))[:1] #solo pre y rec\n",
    "    datos = [acc,acct] + list(prec_rec[0])\n",
    "    accuracy_logit.append(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda superior se ajustó un modelo de Regresión Lineal Regularizada con parámetro <i>C </i> igual a 1, mostrando el efecto de utilizar <i>stemming </i> o <i>lemmatisation</i> en la representación de los datos. Para esto se puede ver que la representación mediante <i>stemming</i> se comporta mejor sobre el test set, produciendo un buen desempeño en todas las métricas presentadas anteriormente (precision, recall, f1-score), además de presentar un desempeño sobre el test set de <b>73,51 %</b>. Esto se explica al igual que en los casos anteriores de Naive Bayes, debido a que este proceso es mejor ya que se trabaja con información pura (palabras en su tronco léxico).\n",
    "\n",
    "El filtrar el proceso de <b>stopwords</b> al igual que en los casos anteriores presenta un mejor comportamiento sobre el test set, debido al mismo razonamiento de que es producido porque en este caso el training set genera una serie de probabilidades en las palabras que elimina el <b>stopwords</b> las cuales benefician al momento de ver el desempeño sobre el test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFECT OF C IN THE MODEL\n",
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.884637\n",
      "Test Accuracy SVM: 0.715170\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.71      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.989589\n",
      "Test Accuracy SVM: 0.723614\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 1.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.710667\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.70      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.702786\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.698565\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.697439\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.70      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Para lo presentado en la celda anterior se tiene un modelo <b>SVM</b> el cual es ajustado al training set analizando su comportamiento en base al parámetro de penalización <i>C</i>, donde este valor representa el error presente en el ajuste del modelo sobre el training set, en otras palabras, representa el grado de suavidad del modelo. Para <i>C</i> pequeños el modelo penaliza menos los errores sobre el ajuste en el training set, traduciéndose en un modelo más suave (</i>soft-margin</i>) el cual permite errores sobre los datos en los cuales se ajusta. Para <i>C</i> grande la penalización sobre el error en el conjunto con el cual se entrena es mayor, traduciéndose en un modelo más estricto (<i>hard-margin</i>) sobre el training set.\n",
    "\n",
    "Para este dataset de opiniones sobre películas se puede ver el efecto esperado del parámetro <i>C</i> sobre el modelo <b>SVM</b>, donde con valores pequeños el desempeño sobre el training set es bajo comparado con otro valor de <i>C</i>. Para <i>C</i> mayor a 1 el modelo es de <i>hard-margin</i>, es decir es estricto sobre el training set, traduciéndose en un desempeño de 100% sobre este set y con un menor desempeño sobre el test set, produciendo un claro <i>overfitting</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy SVM: 0.987901\n",
      "Test Accuracy SVM: 0.738249\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.012099\n",
      "Error (Misclassification) Test: 0.261677\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy SVM: 0.989589\n",
      "Test Accuracy SVM: 0.723614\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.010411\n",
      "Error (Misclassification) Test: 0.276308\n",
      "WITH STEMMING\n",
      "Training Accuracy SVM: 0.981992\n",
      "Test Accuracy SVM: 0.731213\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.73      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.018008\n",
      "Error (Misclassification) Test: 0.268711\n"
     ]
    }
   ],
   "source": [
    "#SEGUN LO DE ARRIBA SE ESCOGE C = 0.1\n",
    "from sklearn.svm import LinearSVC\n",
    "def do_SVM_C(x,y,xt,yt,c=0.1):\n",
    "    model = LinearSVC(C=c)\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "    return model\n",
    "errors_svm = []\n",
    "accuracy_svm = []\n",
    "\n",
    "tipos = [\"normal\",\"lem\",\"stem\"]\n",
    "descripcion = [\"WITHOUT STOP WORDS and WITH LEMM\", \"WITH LEMMATISATION\",\"WITH STEMMING\"]\n",
    "for r, d in zip(tipos,descripcion): #for para mostrar el efecto de distintas representaciones\n",
    "    print d\n",
    "    features_train,labels_train,features_test,labels_test =  representacion(r)\n",
    "    model = do_SVM_C(features_train,labels_train,features_test,labels_test) #es ejecutado SVM retornando el modelo ajustado\n",
    "    \n",
    "    #calculan y guardan los errores de entrenamiento y prueba\n",
    "    error,terror = errors(model,features_train,labels_train,features_test,labels_test) \n",
    "    errors_svm.append([error,terror])\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    \n",
    "    #valores de retornos agregados por datos requeridos en grafico\n",
    "    acc = model.score(features_test,labels_test)\n",
    "    acct = model.score(features_train,labels_train)\n",
    "    prec_rec = precision_recall_fscore_support(labels_test,model.predict(features_test))[:1]\n",
    "    datos = [acc,acct] + list(prec_rec[0])\n",
    "    accuracy_svm.append(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En la celda superior se ajusta un modelo de <b>SVM</b> con parámetro <i>C</i> igual a 0.1 para ver el efecto de realizar distintas representaciones verbales sobre el dataset. Para el caso de la representación según <i>stemming</i> se tiene un mejor desempeño sobre el test set, entregando una precisión de <b>73,12%</b>, esto se atribuye a la misma explicación ya dada anteriormente con los otros modelos.\n",
    "\n",
    "Para el caso de filtrar <b>stopwords</b> en el proceso del modelo este presenta un mejor desempeño sobre el test set, caso contradictorio a lo esperado al igual que en los 3 modelos ya descritos anteriormente, esto deja claro que el training set tiene un sesgo a realizar un ajuste mejor sobre el test set cuando son utilizadas estas palabras que no entregan información. Un ejemplo de este suceso podría presentarse que en el caso en que el conector \"and\" aparezca en una gran cantidad de opiniones <i>positivas</i>, esto ayudará a que el modelo prediga mejor sobre el test set, pero es un caso específico de los dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAI6CAYAAADVFoJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1f2621d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Construir grafico comparativo.\n",
    "import matplotlib.pyplot as plt\n",
    "nostop = errors_bayes[0] + errors_multi[0] + errors_logit[0] + errors_svm[0]\n",
    "lem = errors_bayes[1] + errors_multi[1] + errors_logit[1] + errors_svm[1]\n",
    "stem = errors_bayes[2] + errors_multi[2] + errors_logit[2] + errors_svm[2]\n",
    "\n",
    "#PARA COMPARAR LOS EFECTOS DE STEM Y LEM\n",
    "colors = ['b','r','b','r','b','r','b','r']\n",
    "f, axarr = plt.subplots(2, 2, figsize=(12,8) )\n",
    "barlist = axarr[0, 0].bar(range(8), nostop, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 0].set_title('Without stop words and lem')\n",
    "axarr[0, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 0].set_ylabel('Error')\n",
    "axarr[0, 0].legend(barlist, [\"Training\",\"Test\"], loc=\"center right\", fancybox= True)\n",
    "\n",
    "axarr[0, 1].bar(range(8), lem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 1].set_title('With lem')\n",
    "axarr[0, 1].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 1].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 1].set_ylabel('Error')\n",
    "axarr[0, 1].legend(barlist, [\"Training\",\"Test\"], loc=\"center right\", fancybox= True)\n",
    "\n",
    "axarr[1, 0].bar(range(8), stem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[1, 0].set_title('With stem')\n",
    "axarr[1, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[1, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[1, 0].set_ylabel('Error')\n",
    "axarr[1, 0].legend(barlist, [\"Training\",\"Test\"],loc=\"center right\", fancybox= True)\n",
    "\n",
    "\n",
    "f.tight_layout() #separar los subplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En este ítem se presentan los gráficos que resumen el error según misclassification de todos los modelos ajustados sobre el training set del dataset de <b>Rotten Tomatoes</b>. Donde se asigna un error máximo cuando el modelo se equivoca en predecir una etiqueta, para este caso por ejemplo es cuando el modelo predice una etiqueta como <i>positiva</i> siendo que es <i>negativa</i>. Se puede ver que para el caso en que la representación según  <i>lemmatisation</i>  los modelos Logístico y SVM presentan un <i>overfitting</i>  ya que estos se ajustan mucho al training set, en esta representación el modelo que mejor se comporta presentando el menor error en el test set y con menor overfitting es el de <b>Naive Bayes</b>.\n",
    "\n",
    "Para el caso que se utiliza la representación según <i>stemming</i> los modelos Logístico y SVM también presentan este sobre-ajuste sobre el training set. El modelo que mejor se comporta sobre test set según esta representación es el <b>Multinomial</b>.\n",
    "\n",
    "Se puede observar que cuando se usa lematization <b>sin stopwords</b>, se obtiene un error levemente más bajo, lo cual es contradictorio puesto que como bien se ha mencionado anteriormente, stopword elimina las palabras que no aportan significado a la hora de clasificar, por lo que no aplicar este procedimiento podría producir un sesgo significativo.\n",
    "Para este caso en específico, como no se aplica stopword, el sesgo que se produce justamente ayuda un poco a bajar el error de test, es decir, a clasificar correctamente. Esto se puede explicar ya que ciertas palabras sin significado en el training set, tales como conectores, estén más presentes en clases positivas o negativas según sea el caso, por lo que considerar estas palabras ciertamente provoca una mejora en la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAALKCAYAAADAhJtqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1f26133d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#DECODIFAN LOS DATOS ALMACENADOS EN LAS CELDAS ANTERIORES\n",
    "nostop_ac = accuracy_bayes[0] + accuracy_multi[0] + accuracy_logit[0] + accuracy_svm[0]\n",
    "lem_ac = [accuracy_bayes[1]] + [accuracy_multi[1]] + [accuracy_logit[1]] + [accuracy_svm[1]]\n",
    "stem_ac = [accuracy_bayes[2]] + [accuracy_multi[2]] + [accuracy_logit[2]] + [accuracy_svm[2]]    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy_lem = []\n",
    "accuracy_stem = []\n",
    "for lem,stem in zip(lem_ac,stem_ac):\n",
    "    accuracy_lem += lem[:2]  \n",
    "    accuracy_stem += stem[:2]\n",
    "    precision += [lem[2]]+[stem[2]] \n",
    "    recall+= [lem[3]]+[stem[3]] \n",
    "    \n",
    "#PARA COMPARAR PRECISION Y RECALL\n",
    "f, axarr = plt.subplots(2, 2, figsize=(12,10) )\n",
    "colors = ['#0B9014','#D20BD2']*4\n",
    "barlist = axarr[0, 0].bar(range(8), precision, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 0].set_title('Comparison of metric precision in differents models')\n",
    "axarr[0, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 0].set_ylabel('precision')\n",
    "axarr[0, 0].legend(barlist, [\"Lemmatisation\",\"Stemming\"],loc=\"center right\", fancybox= True)\n",
    "\n",
    "axarr[0, 1].bar(range(8), recall, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 1].set_title('Comparison of metric recall in diferrents models')\n",
    "axarr[0, 1].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 1].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 1].set_ylabel('recall')\n",
    "axarr[0, 1].legend(barlist, [\"Lemmatisation\",\"Stemming\"],loc=\"center right\", fancybox= True)\n",
    "\n",
    "#PARA COMPARAR LOS EFECTOS DE STEM Y LEM\n",
    "colors = ['#014105','#6BB970']*4\n",
    "barlist = axarr[1, 0].bar(range(8), accuracy_lem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[1, 0].set_title('Representation by Lemmatisation')\n",
    "axarr[1, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[1, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[1, 0].set_ylabel('Accuracy')\n",
    "axarr[1, 0].legend(barlist, [\"Test\",\"Training\"], loc=\"center right\", fancybox= True)\n",
    "\n",
    "colors = ['#520052','#FF3CFF']*4\n",
    "barlist = axarr[1, 1].bar(range(8), accuracy_stem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[1, 1].set_title('Representation by Stemming')\n",
    "axarr[1, 1].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[1, 1].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[1, 1].set_ylabel('Accuracy')\n",
    "axarr[1, 1].legend(barlist, [\"Test\",\"Training\"],loc=\"center right\", fancybox= True)\n",
    "\n",
    "\n",
    "f.tight_layout() #separar los subplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Precision y Recall\n",
    "Al analizar la métrica de <b>precision</b> se puede ver que en general ningún método sobresale sobre el resto, teniendo todos un comportamiento similar con la representación mediante <i>lemmatisation</i> y <i>stemming</i>. Como la métrica precisión significa el no etiquetar incorrectamente ejemplos que no son pertenecientes a la clase, se puede ver que esto no es fuertemente dependiente de la representación con la cual se trabaje y tampoco del modelo con el cual se ajustan los datos. Se puede ver que el modelo que tiene mejor <i>precision</i> es el <b>Multinomial</b> con representación mediante <i>stemming</i>.\n",
    "\n",
    "Al analizar la métrica de <b>recall</b> se puede ver al mismo caso que en el gráfico comentado anteriormente ya que ningún método sobresale sobre el resto. Como la métrica recall significa el no etiquetar incorrectamente ejemplos que son pertenecientes a la clase, se puede ver que esto no es fuertemente dependiente de la representación con la cual se trabaje y tampoco del modelo con el cual se ajustan los datos. El modelo que presenta un mejor valor en <i>recall</i> es el <b>Multinomial</b> con representación mediante <i>stemming</i>. Estos gráficos los cuales presentan métricas de desempeño positivo, muestran que en ningún caso la representación mediante <i>lemmatisation</i> logra un mayor desempeño.\n",
    "\n",
    "### Lemmatisation y Stemming\n",
    "Al analizar la <b>accuracy</b> (precisión) de los distintos modelos mediante las distintas representaciones, se puede ver que <i>lemmatisation</i> ofrece valores más altos en la precisión sobre el training set, lo cual en este caso se traduce en un <i>overfitting</i>. La representación mediante <i>stemming</i> posee un comportamiento mejor, ya que los valores de la precisión sobre el test set  son mayores y estos producen un menor <i>overfitting</i> que el caso con <i>lemmatisation</i>.  Se puede concluir en base a los análisis anteriores que la representación mediante <i>stemming</i> es la que mejor se comporta para este dataset, gracias a su filtrado más estricto permitiendo que palabras que significan lo mismo se reduzcan al mismo tronco léxico.\n",
    "\n",
    "\n",
    "Se puede ver que en general los mejores clasificadores para este dataset son los basados en <b>Bayes</b> (Naive Bayes y Naive Bayes Multinomial)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

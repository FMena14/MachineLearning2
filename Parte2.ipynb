{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2)\n",
      "(3554, 2)\n",
      "   Sentiment                                               Text\n",
      "0         -1  everything's serious , poetic , earnest and --...\n",
      "1         -1  narratively , trouble every day is a plodding ...\n",
      "2          1  a truly wonderful tale combined with stunning ...\n",
      "3          1  jason patric and ray liotta make for one splen...\n",
      "4         -1  haneke keeps us at arm's length . guided more ...\n",
      "      Sentiment                                               Text\n",
      "3549          1  a fascinating documentary about the long and e...\n",
      "3550          1  the filmmakers' eye for detail and the high st...\n",
      "3551          1  throwing caution to the wind with an invitatio...\n",
      "3552         -1  ï¿½a big , baggy , sprawling carnival of a movie...\n",
      "3553          1  an incendiary , deeply thought-provoking look ...\n",
      "TRAINING-Cantidad clase negativa:  1784\n",
      "TRAINING-Cantidad clase positiva:  1770\n",
      "TEST-Cantidad clase negativa:  1803\n",
      "TEST-Cantidad clase positiva:  1751\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos y ver sus dimensiones\n",
    "import urllib\n",
    "import pandas as pd\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])\n",
    "print train_df.shape\n",
    "print test_df.shape\n",
    "print train_df.head()\n",
    "print test_df.tail()\n",
    "\n",
    "# Como se puede observar, la dimensionalidad de la data de entrenamiento y la de prueba es de 3554 registros en total.\n",
    "\n",
    "\n",
    "con_neg = 0\n",
    "con_pos = 0\n",
    "\n",
    "for val in train_df[\"Sentiment\"]:\n",
    "    if val > 0:\n",
    "        con_pos+=1\n",
    "    else:\n",
    "        con_neg+=1\n",
    "        \n",
    "#Contar cantidad de cada clase   \n",
    "print \"TRAINING-Cantidad clase negativa: \",train_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TRAINING-Cantidad clase positiva: \",train_df[\"Sentiment\"].tolist().count(1)\n",
    "\n",
    "print \"TEST-Cantidad clase negativa: \",test_df[\"Sentiment\"].tolist().count(-1)\n",
    "print \"TEST-Cantidad clase positiva: \",test_df[\"Sentiment\"].tolist().count(1)\n",
    "\n",
    "# Existen dos clases, textos positivos y negativos. El dataset contiene:\n",
    "\n",
    "# 1784 textos negativos\n",
    "# 1770 textos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------word_extractor---------------------\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " love play game\n",
      " n't love play game\n",
      " eat cake\n",
      " 'm eat cake\n",
      " eat cake\n",
      "---------------------word_extractor_sin_stemming---------------------\n",
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " love play games\n",
      " love playing games\n",
      " loved playing games\n",
      " love playing games\n",
      " n't love playing games\n",
      " eating cake\n",
      " 'm eating cake\n",
      " eat cake\n"
     ]
    }
   ],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def word_extractor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "def word_extractor_sin_stemming(text):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ word.lower() \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "def word_extractor_sin_stop(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    wordtokens = [ stemmer.stem(word.lower()) \\\n",
    "        for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    words = \"\"\n",
    "    for word in wordtokens:\n",
    "        words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"---------------------word_extractor---------------------\"\n",
    "print word_extractor(\"I love to eat cake\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor(\"I love to play games\")\n",
    "print word_extractor(\"I love playing games\")\n",
    "print word_extractor(\"I loved playing the games\")\n",
    "print word_extractor(\"I do not love playing games\")\n",
    "print word_extractor(\"I don't love playing games\")\n",
    "print word_extractor(\"I am eating cake\")\n",
    "print word_extractor(\"I'm eating cake\")\n",
    "print word_extractor(\"I eat cake\")\n",
    "\n",
    "print \"---------------------word_extractor_sin_stemming---------------------\"\n",
    "print word_extractor_sin_stemming(\"I love to eat cake\")\n",
    "print word_extractor_sin_stemming(\"I love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I loved eating the cake\")\n",
    "print word_extractor_sin_stemming(\"I do not love eating cake\")\n",
    "print word_extractor_sin_stemming(\"I don't love eating cake\")\n",
    "\n",
    "# propias\n",
    "print word_extractor_sin_stemming(\"I love to play games\")\n",
    "print word_extractor_sin_stemming(\"I love playing games\")\n",
    "print word_extractor_sin_stemming(\"I loved playing the games\")\n",
    "print word_extractor_sin_stemming(\"I do not love playing games\")\n",
    "print word_extractor_sin_stemming(\"I don't love playing games\")\n",
    "print word_extractor_sin_stemming(\"I am eating cake\")\n",
    "print word_extractor_sin_stemming(\"I'm eating cake\")\n",
    "print word_extractor_sin_stemming(\"I eat cake\")\n",
    "\n",
    "\n",
    "# Se puede observar que al aplicar el algoritmo word_extractor() captura el tronco lexico base de cada palabra\n",
    "# en las distintas oraciones. En los 4 primeros ejemplos se obtiene el mismo tronco lexico para las oraciones,\n",
    "# puesto que se trata solamente de palabras que se le agrega el \"ing\" o el \"ed\" al final.\n",
    "# Tambien se observa que existe diferencia entre poner \"do not\" y \"don't\" obteniendose distinto tronco,\n",
    "# porque en el primer caso se consideran palabras separadas como \"do\" y \"not\" por separado.\n",
    "# Se observa el mismo resultado para las oraciones propias.\n",
    "# Si no se aplica stemming, no todas las palabras quedan en su tronco lexico base, solo se extraen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " love eat cake\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " love play game\n",
      " love playing game\n",
      " loved playing game\n",
      " love playing game\n",
      " n't love playing game\n",
      " eating cake\n",
      " 'm eating cake\n",
      " eat cake\n",
      " travel earth\n"
     ]
    }
   ],
   "source": [
    "# Funcion igual a la anterior, pero con lematizing en vez de stemming\n",
    "\n",
    "def word_extractor2(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) \\\n",
    "            for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "\n",
    "print word_extractor2(\"I love to eat cake\")\n",
    "print word_extractor2(\"I love eating cake\")\n",
    "print word_extractor2(\"I loved eating the cake\")\n",
    "print word_extractor2(\"I do not love eating cake\")\n",
    "print word_extractor2(\"I don't love eating cake\")\n",
    "\n",
    "#propias\n",
    "print word_extractor2(\"I love to play games\")\n",
    "print word_extractor2(\"I love playing games\")\n",
    "print word_extractor2(\"I loved playing the games\")\n",
    "print word_extractor2(\"I do not love playing games\")\n",
    "print word_extractor2(\"I don't love playing games\")\n",
    "print word_extractor2(\"I am eating cake\")\n",
    "print word_extractor2(\"I'm eating cake\")\n",
    "print word_extractor2(\"I eat cake\")\n",
    "print word_extractor2(\"I will travel the earth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras mas frecuentes en el train: \n",
      "566 film\n",
      "481 movie\n",
      "246 one\n",
      "245 like\n",
      "224 ha\n",
      "183 make\n",
      "176 story\n",
      "163 character\n",
      "145 comedy\n",
      "143 time\n",
      "Palabras mas frecuentes en el test: \n",
      "558 film\n",
      "540 movie\n",
      "250 one\n",
      "238 ha\n",
      "230 like\n",
      "197 story\n",
      "175 character\n",
      "165 time\n",
      "161 make\n",
      "134 comedy\n"
     ]
    }
   ],
   "source": [
    "# Representacion vectorial del texto de entrenamiento y el de pruebas\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def representacion(forma):\n",
    "    if forma == \"normal\":\n",
    "        texts_train = [word_extractor_sin_stop(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor_sin_stop(text) for text in test_df.Text]\n",
    "    elif forma == \"stem\":\n",
    "        texts_train = [word_extractor(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor(text) for text in test_df.Text]\n",
    "    elif forma == \"lem\":\n",
    "        texts_train = [word_extractor2(text) for text in train_df.Text]\n",
    "        texts_test = [word_extractor2(text) for text in test_df.Text]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test = vectorizer.transform(texts_test)\n",
    "    labels_train = np.asarray((train_df.Sentiment.astype(float)+1)/2.0) #0 y 1\n",
    "    labels_test = np.asarray((test_df.Sentiment.astype(float)+1)/2.0) # 0 y 1\n",
    "    return features_train,labels_train,features_test,labels_test\n",
    "\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "vocab = vectorizer.get_feature_names() #se crea en base al texts train\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "dist2=list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_train = zip(vocab, dist)\n",
    "lista_train.sort(key=lambda x: x[1])\n",
    "lista_train.reverse()\n",
    "\n",
    "# Se ordenan las palabras por cantidad\n",
    "lista_test = zip(vocab, dist2)\n",
    "lista_test.sort(key=lambda x: x[1])\n",
    "lista_test.reverse()\n",
    "\n",
    "#for tag, count in lista_train:\n",
    "#    print count, tag\n",
    "\n",
    "N = 10\n",
    "\n",
    "pals_train = []\n",
    "pals_test = []\n",
    "\n",
    "print \"Palabras mas frecuentes en el train: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_train[i]\n",
    "    pals_train.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (566 film,481 movie,246 one,245 like,224 ha,183 make,176 story,163 character,145 comedy,143 time)\n",
    "    \n",
    "print \"Palabras mas frecuentes en el test: \"    \n",
    "for i in range(N):\n",
    "    tag, count = lista_test[i]\n",
    "    pals_test.append(tag)\n",
    "    print count,tag\n",
    "    \n",
    "# Mas frecuentes: (558 film,540 movie,250 one,238 ha,230 like,197 story,175 character,165 time,161 make,134 comedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion que evalua el desempeÃ±o de un clasificador generico en el conjunto de entrenamiento y de pruebas\n",
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print(classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "\n",
    "    \n",
    "# Las metricas de classification_report son:\n",
    "# yt: Corresponde a las y de prueba, es decir, las clasificaciones reales.\n",
    "# model.predict(xt): Corresponde a la prediccion de los inputs \"xt\" de prueba, es decir, el y estimado.\n",
    "# target_names: Corresponde a una lista de strings para mostrar nombres para las etiquetas. En este caso \"+\" y \"-\"\n",
    "\n",
    "#Funcion que calcula los errores de un modelo\n",
    "def errors(model,x,y,xt,yt): \n",
    "    yhat = model.predict(x)\n",
    "    yhat_test = model.predict(xt)\n",
    "    error = mis_class(yhat,y)\n",
    "    terror = mis_class(yhat_test,yt)\n",
    "    return error, terror\n",
    "\n",
    "def mis_class(yhat,y):\n",
    "    miss = [ 1 if(i != j) else 0  \n",
    "            for i,j in zip(yhat,y)]\n",
    "    return np.mean(miss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy BernoulliNB: 0.938098\n",
      "Test Accuracy BernoulliNB: 0.762173\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.77      0.76      0.77      1803\n",
      "          -       0.76      0.76      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.061902\n",
      "Error (Misclassification) Test: 0.237760\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.041362\n",
      "Error (Misclassification) Test: 0.261396\n",
      "WITH STEMMING\n",
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057119\n",
      "Error (Misclassification) Test: 0.252110\n",
      "[ 0.0631459  0.9368541] shanghai ghetto should be applauded for finding a new angle on a tireless story , but you might want to think twice before booking passage .\n",
      "\n",
      "[ 0.48359847  0.51640153] the tone shifts abruptly from tense to celebratory to soppy .\n",
      "\n",
      "[ 0.00922985  0.99077015] so refreshingly incisive is grant that for the first time he'll probably appeal more to guys than to their girlfriends who drag them to this movie for the hugh factor .\n",
      "\n",
      "[ 0.82028989  0.17971011] wasabi is slight fare indeed , with the entire project having the feel of something tossed off quickly ( like one of hubert's punches ) , but it should go down smoothly enough with popcorn .\n",
      "\n",
      "[ 0.8952497  0.1047503] just too silly and sophomoric to ensnare its target audience .\n",
      "\n",
      "[ 0.8337316  0.1662684] the entire movie is in need of a scented bath .\n",
      "\n",
      "[ 0.34457771  0.65542229] the filmmaker ascends , literally , to the olympus of the art world , but he would have done well to end this flawed , dazzling series with the raising of something other than his own cremaster .\n",
      "\n",
      "[ 0.32623085  0.67376915] report card : doesn't live up to the exalted tagline - there's definite room for improvement . doesn't deserve a passing grade ( even on a curve ) .\n",
      "\n",
      "[ 0.13562907  0.86437093] harrison's flowers puts its heart in the right place , but its brains are in no particular place at all .\n",
      "\n",
      "[ 0.38524902  0.61475098] on its own cinematic terms , it successfully showcases the passions of both the director and novelist byatt .\n",
      "\n",
      "[ 0.24219159  0.75780841] the film may appear naked in its narrative form . . . but it goes deeper than that , to fundamental choices that include the complexity of the catholic doctrine\n",
      "\n",
      "[ 0.0288199  0.9711801] it's a terrific american sports movie and dennis quaid is its athletic heart .\n",
      "\n",
      "[ 0.03564709  0.96435291] kinnear gives a tremendous performance .\n",
      "\n",
      "[ 0.75824332  0.24175668] trapped won't score points for political correctness , but it may cause parents a few sleepless hours -- a sign of its effectiveness .\n",
      "\n",
      "[ 0.62011384  0.37988616] some of it is clever , but it is never melodic/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import random\n",
    "\n",
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "errors_bayes = []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_bayes.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n",
    "    \n",
    "# Las StopWords son el nombre que se le da a todas aquellas palabras que no tienen ningÃºn atributo\n",
    "# de bÃºsqueda, es decir, son palabras de significado vacÃ­o como los artÃ­culos, los pronombres o las preposiciones.\n",
    "# La importancia de borrar estas palabras es para hacer mas eficiente el analisis de clasificacion, puesto que \n",
    "# asi no se pierde tiempo procesando y guardando estas palabras en el algoritmo.\n",
    "\n",
    "#Precision tasa/razon entre los true positive y el resto (true positive + false positive), es decir, clases asignadas correctamente\n",
    "#Recall tasa/razon entre los true positive y el resto del real positivo (true positive + false negative)\n",
    "#f1-score el promedio harmonico/ponderado entre precision y recall\n",
    "#support cantidad de ejemplos asignadas a cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy MULTINOMIAL: 0.940630\n",
      "Test Accuracy MULTINOMIAL: 0.759921\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.77      0.76      1803\n",
      "          -       0.76      0.75      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.059370\n",
      "Error (Misclassification) Test: 0.240011\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy MULTINOMIAL: 0.959482\n",
      "Test Accuracy MULTINOMIAL: 0.740782\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.040518\n",
      "Error (Misclassification) Test: 0.259145\n",
      "WITH STEMMING\n",
      "Training Accuracy MULTINOMIAL: 0.942319\n",
      "Test Accuracy MULTINOMIAL: 0.749789\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.057681\n",
      "Error (Misclassification) Test: 0.250141\n",
      "[ 0.98412662  0.01587338] if it tried to do anything more , it would fail and perhaps explode , but at this level of manic whimsy , it is just about right .\n",
      "\n",
      "[ 0.9835296  0.0164704] formula 51 is so trite that even yu's high-energy action stylings can't break through the stupor .\n",
      "\n",
      "[ 0.96449884  0.03550116] the movie's blatant derivativeness is one reason it's so lackluster .\n",
      "\n",
      "[ 0.07907176  0.92092824] [westbrook] makes a wonderful subject for the camera .\n",
      "\n",
      "[ 0.94658659  0.05341341] the movie takes itself too seriously and , as a result , it makes for only intermittent fun .\n",
      "\n",
      "[ 0.68772269  0.31227731] like an afterschool special with costumes by gianni versace , mad love looks better than it feels .\n",
      "\n",
      "[ 0.98379912  0.01620088] for something as splendid-looking as this particular film , the viewer expects something special but instead gets [sci-fi] rehash .\n",
      "\n",
      "[ 0.84473096  0.15526904] until its final minutes this is a perceptive study of two families in crisis -- and of two girls whose friendship is severely tested by bad luck and their own immaturity .\n",
      "\n",
      "[ 0.59568518  0.40431482] hip-hop rarely comes alive as its own fire-breathing entity in this picture .\n",
      "\n",
      "[ 0.49201702  0.50798298] we've seen the hippie-turned-yuppie plot before , but there's an enthusiastic charm in <i ? fire that makes the formula fresh again .\n",
      "\n",
      "[ 0.01383729  0.98616271] a film that's flawed and brilliant in equal measure .\n",
      "\n",
      "[ 0.21054868  0.78945132] there's more repetition than creativity throughout the movie .\n",
      "\n",
      "[ 0.29874061  0.70125939] as giddy and whimsical and relevant today as it was 270 years ago .\n",
      "\n",
      "[ 0.78434957  0.21565043] it's all very cute , though not terribly funny if you're more than six years old .\n",
      "\n",
      "[ 0.19235323  0.80764677] ultimately too repellent to fully endear itself to american art house audiences , but it is notable for its stylistic austerity and forcefulness .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "\n",
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "\n",
    "errors_multi= []\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "error,terror = errors(model,features_train,labels_train,features_test,labels_test)\n",
    "errors_multi.append([error,terror])\n",
    "print \"Error (Misclassification) Training: %f\"%(error)\n",
    "print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "\n",
    "test_pred = model.predict_proba(features_test)\n",
    "spl = random.sample(xrange(len(test_pred)), 15)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFECT OF C IN THE MODEL\n",
      "Usando C= 0.010000\n",
      "Error (Misclassification) Training: 0.215532\n",
      "Error (Misclassification) Test: 0.321047\n",
      "Training Accuracy LOGISTIC: 0.784468\n",
      "Test Accuracy LOGISTIC: 0.678863\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.67      0.73      0.70      1803\n",
      "          -       0.69      0.63      0.66      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "Usando C= 0.100000\n",
      "Error (Misclassification) Training: 0.107766\n",
      "Error (Misclassification) Test: 0.280810\n",
      "Training Accuracy LOGISTIC: 0.892234\n",
      "Test Accuracy LOGISTIC: 0.719111\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.72      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Usando C= 10.000000\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.281373\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.718548\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Usando C= 100.000000\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.285875\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.714044\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.72      1803\n",
      "          -       0.71      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "Usando C= 1000.000000\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.287563\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.712356\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.71      0.71      1803\n",
      "          -       0.70      0.72      0.71      1751\n",
      "\n",
      "avg / total       0.71      0.71      0.71      3554\n",
      "\n",
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.735998\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.263928\n",
      "WITH LEMMATISATION\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.718548\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.281373\n",
      "WITH STEMMING\n",
      "Training Accuracy LOGISTIC: 0.999719\n",
      "Test Accuracy LOGISTIC: 0.725303\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "Error (Misclassification) Training: 0.000281\n",
      "Error (Misclassification) Test: 0.274620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C= %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        error,terror = errors(model,x,y,xt,yt)\n",
    "        print \"Error (Misclassification) Training: %f\"%(error)\n",
    "        print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "def do_LOGIT_C(x,y,xt,yt,c):\n",
    "    start_t = time.time()\n",
    "    model = LogisticRegression(penalty='l2',C=c)\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    error,terror = errors(model,x,y,xt,yt)\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    return error,terror\n",
    "\n",
    "\n",
    "errors_logit = []\n",
    "\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "#SEGUN LO DE ARRIBA SE ESCOGE C = ...\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_logit.append([error,terror])\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_logit.append([error,terror])\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "error,terror = do_LOGIT_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_logit.append([error,terror])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.873382\n",
      "Test Accuracy SVM: 0.729243\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.75      0.74      1803\n",
      "          -       0.73      0.71      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.981992\n",
      "Test Accuracy SVM: 0.731213\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.73      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.701942\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.72      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.700535\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.700535\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "EFFECT OF C IN THE MODEL\n",
      "El valor de C que se esta probando: 0.010000\n",
      "Training Accuracy SVM: 0.884637\n",
      "Test Accuracy SVM: 0.715170\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.71      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 0.100000\n",
      "Training Accuracy SVM: 0.989589\n",
      "Test Accuracy SVM: 0.723614\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.73      1803\n",
      "          -       0.72      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "El valor de C que se esta probando: 10.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.702786\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 100.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.698565\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "El valor de C que se esta probando: 1000.000000\n",
      "Training Accuracy SVM: 1.000000\n",
      "Test Accuracy SVM: 0.697439\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.70      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "WITHOUT STOP WORDS and WITH LEMM\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.282499\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.717422\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.70      0.72      1803\n",
      "          -       0.70      0.73      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "WITH LEMMATISATION\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.297130\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.702786\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.71      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n",
      "WITH STEMMING\n",
      "Error (Misclassification) Training: 0.000000\n",
      "Error (Misclassification) Test: 0.297974\n",
      "Training Accuracy LOGISTIC: 1.000000\n",
      "Test Accuracy LOGISTIC: 0.701942\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.71      0.69      0.70      1803\n",
      "          -       0.69      0.72      0.70      1751\n",
      "\n",
      "avg / total       0.70      0.70      0.70      3554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "        \n",
    "def do_SVM_C(x,y,xt,yt,c):\n",
    "    model = LinearSVC(C=c)\n",
    "    model = model.fit(x, y)\n",
    "    error,terror = errors(model,x,y,xt,yt)\n",
    "    print \"Error (Misclassification) Training: %f\"%(error)\n",
    "    print \"Error (Misclassification) Test: %f\"%(terror)\n",
    "    score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    return error,terror\n",
    "\n",
    "errors_svm = []\n",
    "\n",
    "print \"EFFECT OF C IN THE MODEL\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "#SEGUN LO DE ARRIBA SE ESCOGE C = ...\n",
    "print \"WITHOUT STOP WORDS and WITH LEMM\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"normal\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_svm.append([error,terror])\n",
    "\n",
    "print \"WITH LEMMATISATION\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"lem\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_svm.append([error,terror])\n",
    "\n",
    "print \"WITH STEMMING\"\n",
    "features_train,labels_train,features_test,labels_test = representacion(\"stem\")\n",
    "error,terror = do_SVM_C(features_train,labels_train,features_test,labels_test,10)\n",
    "errors_svm.append([error,terror])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGCCAYAAAAVLEKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff181ded90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Construir grafico comparativo. Probablemente de errores de entrenamiento y test\n",
    "\n",
    "def results_model(model,x,y,xt,yt):\n",
    "    model = model.fit(x, y)\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    return acc_tr,acc_test\n",
    "\n",
    "\n",
    "#do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "#do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "#do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "#do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "nostop = errors_bayes[0] + errors_multi[0] + errors_logit[0] + errors_svm[0]\n",
    "lem = errors_bayes[0] + errors_multi[1] + errors_logit[1] + errors_svm[1]\n",
    "stem = errors_bayes[2] + errors_multi[2] + errors_logit[2] + errors_svm[2]\n",
    "\n",
    "#PARA COMPARAR LOS EFECTOS DE STEM Y LEM\n",
    "colors = ['b','r','b','r','b','r','b','r']\n",
    "f, axarr = plt.subplots(2, 2)\n",
    "barlist = axarr[0, 0].bar(range(8), nostop, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 0].set_title('Without stop words and lem')\n",
    "axarr[0, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 0].set_ylabel('Error')\n",
    "axarr[0, 0].legend(barlist, [\"Test\",\"Training\"])\n",
    "\n",
    "axarr[0, 1].bar(range(8), lem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[0, 1].set_title('With lem')\n",
    "axarr[0, 1].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[0, 1].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[0, 1].set_ylabel('Error')\n",
    "axarr[0, 1].legend(barlist, [\"Test\",\"Training\"])\n",
    "\n",
    "axarr[1, 0].bar(range(8), stem, width  = 0.5, align = \"center\", color = colors)\n",
    "axarr[1, 0].set_title('With stem')\n",
    "axarr[1, 0].set_xticks([0.5,2.5,4.5,6.5])\n",
    "axarr[1, 0].set_xticklabels(('Naive Bayes', 'Multinomial', 'Logistic','SVM'))\n",
    "axarr[1, 0].set_ylabel('Error')\n",
    "axarr[1, 0].legend(barlist, [\"Test\",\"Training\"],loc=2)\n",
    "\n",
    "\n",
    "f.tight_layout() #separar los subplot\n",
    "leg = plt.legend(loc='upper right', fancybox=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
